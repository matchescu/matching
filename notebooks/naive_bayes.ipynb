{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Entity Resolution using the Naive Bayes ML model\n",
    "\n",
    "We're going to dive into using the Naive Bayes regression model. This model\n",
    "is probabilistic in nature and can be trained to classify data in a given number\n",
    "of categories.\n",
    "The model is supervised, meaning that it learns using known good examples.\n",
    "Sounds like an ideal fit for entity resolution, right?\n",
    "\n",
    "Let's dive in!\n",
    "\n",
    "_(btw, the first parts of this code are the same as the Fellegi-Sunter example\n",
    "notebook, so check that one out, too)_.   "
   ],
   "id": "940700d100283c33"
  },
  {
   "cell_type": "code",
   "id": "ca31fd0d-16e8-4b69-ae32-3cfa3d13cd64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:00:09.208321Z",
     "start_time": "2024-10-09T07:00:08.900742Z"
    }
   },
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from typing import Generator\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from matchescu.matching.entity_reference import (\n",
    "    EntityReferenceComparisonConfig,\n",
    "    NaiveBayesComparison,\n",
    ")\n",
    "from matchescu.matching.ml.datasets import CsvDataSource, Traits"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we need to define some fairly important 'constants' (alas, there's no such thing in Python).\n",
    "Feel free to change these values to whichever dataset you want to test."
   ],
   "id": "86ec9eb605aa9917"
  },
  {
   "cell_type": "code",
   "id": "ca59198f-45a6-48da-b119-b1e5d18d81a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:00:11.529122Z",
     "start_time": "2024-10-09T07:00:11.525257Z"
    }
   },
   "source": [
    "DATADIR = os.path.abspath(\"../../data\")\n",
    "LEFT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Abt.csv\")\n",
    "RIGHT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Buy.csv\")\n",
    "GROUND_TRUTH_PATH = os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The sources of information can be structured in any way. However, when we read\n",
    "from the data source we expect to be able to refer to discrete pieces of\n",
    "information.\n",
    "The important bit is to have a decent feature extraction process that is able to\n",
    "produce relatively uniformly shaped entity references. That's what `Traits()`\n",
    "do. That way we can get a neat matching process going.  "
   ],
   "id": "94a19e6978f60a76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:00:13.869296Z",
     "start_time": "2024-10-09T07:00:13.860381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set up abt extraction\n",
    "abt_traits = list(Traits().int([0]).string([1, 2]).currency([3]))\n",
    "abt = CsvDataSource(name=\"abt\", traits=abt_traits).read_csv(LEFT_CSV_PATH)\n",
    "# set up buy extraction\n",
    "buy_traits = list(Traits().int([0]).string([1, 2, 3]).currency([4]))\n",
    "buy = CsvDataSource(name=\"buy\", traits=buy_traits).read_csv(RIGHT_CSV_PATH)\n",
    "# set up ground truth\n",
    "gt = set(\n",
    "    pl.read_csv(\n",
    "        os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\"),\n",
    "        ignore_errors=True,\n",
    "    ).iter_rows()\n",
    ")"
   ],
   "id": "83b92f4b0ad5b632",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So far, this is very similar to the setup we had for the Fellegi-Sunter model.\n",
    "It's time to introduce the twist required to use the Naive Bayes model."
   ],
   "id": "7256f6f27121c8a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:01:07.519832Z",
     "start_time": "2024-10-09T07:01:07.516288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cmp_config = (\n",
    "    NaiveBayesComparison()\n",
    "    .levenshtein(\"name\", 1, 1, threshold=0.8)\n",
    "    .levenshtein(\"description\", 2, 2, threshold=0.8)\n",
    "    .exact(\"price\", 3, 4)\n",
    ")"
   ],
   "id": "9a8c40f7a87a6ed5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, the setup is still very similar to the Fellegi-Sunter model.\n",
    "We can even reuse our `RecordLinkageDataSet` to showcase the Naive Bayes model."
   ],
   "id": "f119313d74584337"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:29:46.721186Z",
     "start_time": "2024-10-09T08:28:09.803252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml.datasets import RecordLinkageDataSet\n",
    "\n",
    "ds = RecordLinkageDataSet(abt, buy, gt).attr_compare(cmp_config).cross_sources()\n",
    "y = ds.target_vector.to_numpy()\n",
    "X = ds.feature_matrix.to_numpy()"
   ],
   "id": "fbeabd2e052f94d2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We just created the same type of feature matrix and target vector like the ones\n",
    "we had for the Fellegi-Sunter model.\n",
    "This time around, however, we need to pass them to SciKit Learn so we'll need\n",
    "all our data to be `numpy.ndarray`s.\n",
    "Also, the feature matrix values will be in the set `{-1, 1}`. "
   ],
   "id": "275e706feb9e71c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:39:28.245767Z",
     "start_time": "2024-10-09T08:39:28.193928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6)\n",
    "y_ratio = len(y[y == 1]) / len(y)\n",
    "y_train_ratio = len(y_train[y_train == 1]) / len(y_train)\n",
    "y_test_ratio = len(y_test[y_test == 1]) / len(y_test)\n",
    "print(y_ratio, y_train_ratio, y_test_ratio)"
   ],
   "id": "1937dd3ee913f9cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009293050458637878 0.00091631592991948 0.0009487887060258672\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now train the Naive Bayes model that ships with SciKit learn.",
   "id": "71650989f15c1e09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:39:32.303956Z",
     "start_time": "2024-10-09T08:39:32.222571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model = model.fit(X_train, y_train)"
   ],
   "id": "e45a9e9214087a12",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Much easier than the Fellegi-Sunter method. Let's make predictions and compute\n",
    "precision, recall and the F1 score."
   ],
   "id": "f9d5564cdf5d97ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:39:35.134196Z",
     "start_time": "2024-10-09T08:39:35.092687Z"
    }
   },
   "cell_type": "code",
   "source": "y_pred = model.predict(X_test)",
   "id": "41d5a203a0b3e7b2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can compute our metrics.",
   "id": "18e713f96a0bbccb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:39:36.566560Z",
     "start_time": "2024-10-09T08:39:36.560688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tp = np.sum(np.logical_and(y_pred == 1, y_test == 1))\n",
    "fp = np.sum(np.logical_and(y_pred == 1, y_test == 0))\n",
    "tn = np.sum(np.logical_and(y_pred == 0, y_test == 0))\n",
    "fn = np.sum(np.logical_and(y_pred == 0, y_test == 1))\n",
    "total = len(y_test)\n",
    "print(f\"total comparisons: {total}\")\n",
    "print(f\"tp={tp};fp={fp};tn={tn};fn={fn}\")\n",
    "\n",
    "p = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "r = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "print(f\"precision={p}\", f\"recall={r}\", f\"F1={f1}\")"
   ],
   "id": "433106402b5eccb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total comparisons: 472181\n",
      "tp=43;fp=149;tn=471584;fn=405\n",
      "precision=0.22395833333333334 recall=0.09598214285714286 F1=0.134375\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Naive-Bayes model has a different approach to determining the $\\mu$ and\n",
    "$\\lambda$ error margins for definitive match and non-match, respectively.\n",
    "It yields slightly better results compared to the classical Fellegi-Sunter\n",
    "deterministic decision model. It also involves writing much less code and there\n",
    "are still many ways of improving the results. A more subtle note is that this\n",
    "model yields more balanced precision/recall which makes it a better candidate\n",
    "for improvements via attribute similarity (because it is more sensitive by\n",
    "default).\n",
    "\n",
    "To improve our results, we can try various probabilistic distributions (we're\n",
    "using the normal distribution here to make decisions, but we could very well try\n",
    "out the Bernoulli distribution). Since the training is so fast, we can actually\n",
    "find the best fit for our data by iterating over many options. We know full well\n",
    "that the resulting model is overfitted and won't transfer to other data, but at\n",
    "least we get a very tunable evaluator fairly quickly and easily. \n",
    "\n",
    "Before we get into the weeds, there's something obviously wrong with this model:\n",
    "it doesn't even capture the case when data is missing. So maybe a better way\n",
    "forward is to capture more information in each individual feature. That starts\n",
    "by having floating point values in the feature matrix.\n",
    "\n",
    "Enter logistic regression!"
   ],
   "id": "635b982829490e88"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
