{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Using Raw String Similarity Features\n",
    "\n",
    "The paper [@bilenko2002learning] outlines a process by which we can learn the\n",
    "edit distance between two strings using minimal training over pairs of\n",
    "coreferent strings.\n",
    "Coreferent strings are strings found in references to the same entity.\n",
    "The idea is to fine tune the algorithm that computes the string edit distance to\n",
    "the business domain where we perform entity resolution. By doing so we increase\n",
    "the chances of recognizing records that refer to the same entity more\n",
    "accurately.\n",
    "The paper also references an algorithm [@ristad1998learning] which learns to\n",
    "compute the Levenshtein distance between two strings from training data. The\n",
    "method is computationally intensive and other methods have superseded it.\n",
    "\n",
    "Another finding suggests that accuracy gains can be made by training the SVM\n",
    "kernel that performs entity resolution in a specific way. This notebook goes\n",
    "through that training procedure and compares the results using SVMs to those\n",
    "obtained using Logistic Regression."
   ],
   "id": "b9239db8823ba023"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T14:19:55.568710Z",
     "start_time": "2024-12-13T14:19:55.565978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "DATADIR = os.path.abspath(\"../../data\")\n",
    "CORA_PATH = os.path.join(DATADIR, \"cora\", \"cora.csv\")\n",
    "\n",
    "LEFT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Abt.csv\")\n",
    "RIGHT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Buy.csv\")\n",
    "GROUND_TRUTH_PATH = os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\")"
   ],
   "id": "9bcd2799fcf96fd9",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T14:19:55.605966Z",
     "start_time": "2024-12-13T14:19:55.596790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml.datasets import Traits, CsvDataSource\n",
    "\n",
    "cora = (\n",
    "    pl.read_csv(CORA_PATH, has_header=False, ignore_errors=True)\n",
    "    .rename(\n",
    "        {\n",
    "            \"column_1\": \"id\",\n",
    "            \"column_3\": \"class\",\n",
    "            \"column_4\": \"author\",\n",
    "            \"column_5\": \"volume\",\n",
    "            \"column_6\": \"title\",\n",
    "            \"column_7\": \"institution\",\n",
    "            \"column_8\": \"venue\",\n",
    "            \"column_11\": \"year\",\n",
    "        }\n",
    "    )\n",
    "    .select(pl.col(\"id\", \"class\", \"author\", \"title\", \"venue\", \"year\"))\n",
    ")\n",
    "\n",
    "# set up abt extraction\n",
    "abt_traits = list(Traits().int([0]).string([1, 2]).currency([3]))\n",
    "abt = CsvDataSource(name=\"abt\", traits=abt_traits).read_csv(LEFT_CSV_PATH)\n",
    "# set up buy extraction\n",
    "buy_traits = list(Traits().int([0]).string([1, 2, 3]).currency([4]))\n",
    "buy = CsvDataSource(name=\"buy\", traits=buy_traits).read_csv(RIGHT_CSV_PATH)\n",
    "# set up ground truth\n",
    "gt = set(\n",
    "    pl.read_csv(\n",
    "        os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\"),\n",
    "        ignore_errors=True,\n",
    "    ).iter_rows()\n",
    ")"
   ],
   "id": "b9741bb63d0c5d05",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's set up a couple of functions that will help us shortly.\n",
    "\n",
    "We need to be able to perform a k-fold split of the training/test data to\n",
    "reproduce the results for classic edit distance from the paper\n",
    "[@bilenko2002learning].\n",
    "\n",
    "We will use the Levenshtein distance between coreferent strings to train an\n",
    "SVM kernel and a Logistic regression model. We wouldn't be able to use an NB\n",
    "approach or the classic pattern matching here since those don't work with\n",
    "continuous feature values."
   ],
   "id": "d001811f44c60ae3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T14:19:55.611600Z",
     "start_time": "2024-12-13T14:19:55.608313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jellyfish import levenshtein_distance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "def generate_k_folds(\n",
    "    training_features: np.ndarray, training_target: np.ndarray, n_splits: int\n",
    ") -> list[tuple]:\n",
    "    kf = KFold(n_splits)\n",
    "    result = []\n",
    "    for train_idx, cv_idx in kf.split(training_features, training_target):\n",
    "        result.append(\n",
    "            (\n",
    "                training_features[train_idx],\n",
    "                training_target[train_idx],\n",
    "                training_features[cv_idx],\n",
    "                training_target[cv_idx],\n",
    "            )\n",
    "        )\n",
    "    return result\n",
    "\n",
    "\n",
    "def train_svm(features, target):\n",
    "    svm_kernel = LinearSVC()\n",
    "    svm_kernel.fit(features, target)\n",
    "    return svm_kernel\n",
    "\n",
    "\n",
    "def train_regression(features, target):\n",
    "    log_regression = LogisticRegression()\n",
    "    log_regression.fit(features, target)\n",
    "    return log_regression\n",
    "\n",
    "\n",
    "def _safe_str(value) -> str:\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    if not isinstance(value, str):\n",
    "        return str(value)\n",
    "    return value\n",
    "\n",
    "\n",
    "def compute_distances(values: tuple) -> tuple:\n",
    "    values = tuple(map(_safe_str, values))\n",
    "    return (\n",
    "        levenshtein_distance(values[2], values[8]),\n",
    "        levenshtein_distance(values[3], values[9]),\n",
    "        levenshtein_distance(values[4], values[10]),\n",
    "    )"
   ],
   "id": "970ed70d018355a7",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now let's set the stable global parameters:\n",
    "\n",
    "- the fold count (i.e. the number of folds we're going to take through the input data)\n",
    "- the models (i.e. a mapping of user-friendly model name to training function)"
   ],
   "id": "26298c7a5a35eea0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T14:19:59.782990Z",
     "start_time": "2024-12-13T14:19:55.623550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "fold_count = int(input(\"folds:\"))\n",
    "models = {\"svm\": train_svm, \"regression\": train_regression}"
   ],
   "id": "839f3e67bfdbc056",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For the deduplication scenario, we want to both reproduce the findings of Milenko et al\n",
    "and test how logistic regression compares to SVM.\n",
    "\n",
    "We start by preparing a data deduplication dataframe."
   ],
   "id": "26f4c8842c3d3685"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T14:21:28.062802Z",
     "start_time": "2024-12-13T14:19:59.806980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "self_join = cora.join(cora, how=\"cross\")\n",
    "y = self_join.select(pl.nth(1) == pl.nth(len(cora.columns) + 1)).to_numpy().ravel()\n",
    "X = self_join.map_rows(compute_distances).to_numpy()\n",
    "print(X.shape, y.shape)"
   ],
   "id": "ac4da705115d12d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1671849, 3) (1671849,)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we evaluate the two models on the data deduplication dataframe.",
   "id": "9ff5af02253d403e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T14:23:09.808313Z",
     "start_time": "2024-12-13T14:22:58.660581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "folds = generate_k_folds(X_train, y_train, fold_count)\n",
    "stats = []\n",
    "for idx, (\n",
    "    X_fold_train,\n",
    "    y_fold_train,\n",
    "    X_fold_validation,\n",
    "    y_fold_validation,\n",
    ") in enumerate(folds, start=1):\n",
    "    trained_models = {\n",
    "        model: train(X_fold_train, y_fold_train) for model, train in models.items()\n",
    "    }\n",
    "    print(\"trained on fold #\", idx)\n",
    "\n",
    "    for model_name, model in trained_models.items():\n",
    "        prediction = model.predict(X_fold_validation)\n",
    "        f1 = f1_score(y_fold_validation, prediction)\n",
    "        stats.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"precision\": precision_score(y_fold_validation, prediction),\n",
    "                \"recall\": recall_score(y_fold_validation, prediction),\n",
    "                \"f1\": f1,\n",
    "            }\n",
    "        )\n",
    "    print(\"evaluated fold #\", idx)\n",
    "\n",
    "stats = pl.DataFrame(stats)\n",
    "display(stats)"
   ],
   "id": "f4232d883c8b2879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained on fold # 1\n",
      "evaluated fold # 1\n",
      "trained on fold # 2\n",
      "evaluated fold # 2\n",
      "trained on fold # 3\n",
      "evaluated fold # 3\n",
      "trained on fold # 4\n",
      "evaluated fold # 4\n",
      "trained on fold # 5\n",
      "evaluated fold # 5\n",
      "trained on fold # 6\n",
      "evaluated fold # 6\n",
      "trained on fold # 7\n",
      "evaluated fold # 7\n",
      "trained on fold # 8\n",
      "evaluated fold # 8\n",
      "trained on fold # 9\n",
      "evaluated fold # 9\n",
      "trained on fold # 10\n",
      "evaluated fold # 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "shape: (20, 4)\n",
       "┌────────────┬───────────┬──────────┬──────────┐\n",
       "│ model      ┆ precision ┆ recall   ┆ f1       │\n",
       "│ ---        ┆ ---       ┆ ---      ┆ ---      │\n",
       "│ str        ┆ f64       ┆ f64      ┆ f64      │\n",
       "╞════════════╪═══════════╪══════════╪══════════╡\n",
       "│ svm        ┆ 0.767399  ┆ 0.823154 ┆ 0.794299 │\n",
       "│ regression ┆ 0.782952  ┆ 0.792016 ┆ 0.787458 │\n",
       "│ svm        ┆ 0.768405  ┆ 0.805143 ┆ 0.786345 │\n",
       "│ regression ┆ 0.780032  ┆ 0.772198 ┆ 0.776095 │\n",
       "│ svm        ┆ 0.748223  ┆ 0.805802 ┆ 0.775946 │\n",
       "│ …          ┆ …         ┆ …        ┆ …        │\n",
       "│ regression ┆ 0.775828  ┆ 0.782067 ┆ 0.778935 │\n",
       "│ svm        ┆ 0.773529  ┆ 0.831292 ┆ 0.801371 │\n",
       "│ regression ┆ 0.789762  ┆ 0.798499 ┆ 0.794106 │\n",
       "│ svm        ┆ 0.753534  ┆ 0.828287 ┆ 0.789144 │\n",
       "│ regression ┆ 0.769762  ┆ 0.799203 ┆ 0.784206 │\n",
       "└────────────┴───────────┴──────────┴──────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>precision</th><th>recall</th><th>f1</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;svm&quot;</td><td>0.767399</td><td>0.823154</td><td>0.794299</td></tr><tr><td>&quot;regression&quot;</td><td>0.782952</td><td>0.792016</td><td>0.787458</td></tr><tr><td>&quot;svm&quot;</td><td>0.768405</td><td>0.805143</td><td>0.786345</td></tr><tr><td>&quot;regression&quot;</td><td>0.780032</td><td>0.772198</td><td>0.776095</td></tr><tr><td>&quot;svm&quot;</td><td>0.748223</td><td>0.805802</td><td>0.775946</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;regression&quot;</td><td>0.775828</td><td>0.782067</td><td>0.778935</td></tr><tr><td>&quot;svm&quot;</td><td>0.773529</td><td>0.831292</td><td>0.801371</td></tr><tr><td>&quot;regression&quot;</td><td>0.789762</td><td>0.798499</td><td>0.794106</td></tr><tr><td>&quot;svm&quot;</td><td>0.753534</td><td>0.828287</td><td>0.789144</td></tr><tr><td>&quot;regression&quot;</td><td>0.769762</td><td>0.799203</td><td>0.784206</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T14:23:15.757703Z",
     "start_time": "2024-12-13T14:23:15.754362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "avg_stats = stats.group_by(\"model\").mean()\n",
    "display(avg_stats)"
   ],
   "id": "5d6e75d34086aff1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (2, 4)\n",
       "┌────────────┬───────────┬──────────┬──────────┐\n",
       "│ model      ┆ precision ┆ recall   ┆ f1       │\n",
       "│ ---        ┆ ---       ┆ ---      ┆ ---      │\n",
       "│ str        ┆ f64       ┆ f64      ┆ f64      │\n",
       "╞════════════╪═══════════╪══════════╪══════════╡\n",
       "│ svm        ┆ 0.760782  ┆ 0.819162 ┆ 0.788859 │\n",
       "│ regression ┆ 0.776637  ┆ 0.789554 ┆ 0.783011 │\n",
       "└────────────┴───────────┴──────────┴──────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>precision</th><th>recall</th><th>f1</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;svm&quot;</td><td>0.760782</td><td>0.819162</td><td>0.788859</td></tr><tr><td>&quot;regression&quot;</td><td>0.776637</td><td>0.789554</td><td>0.783011</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We've managed to reproduce similar results as the ones obtained in [@bilenko2002learning]\n",
    "for the SVM kernel. We notice that logistic regression and SVMs perform similarly.\n",
    "\n",
    "Our next task is to evaluate SVMs and Logistic Regression on the Abt-Buy dataset.\n",
    "This dataset is much more challenging than the Cora dataset so we expect lower\n",
    "scores, in line with the scores we obtained in our other notebooks.\n",
    "\n",
    "In contrast to the other notebooks so far, we will stick to comparing strings.\n",
    "Except we'll permute the coreferences across name, manufacturer and description\n",
    "gaining much more information."
   ],
   "id": "2619749b60cc039a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T14:35:35.022966Z",
     "start_time": "2024-12-13T14:31:22.017270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml.datasets import RecordLinkageDataSet\n",
    "from matchescu.matching.entity_reference import RawComparison\n",
    "\n",
    "cmp_config = (\n",
    "    RawComparison()\n",
    "    .levenshtein_distance(\"name\", 1, 1)\n",
    "    .levenshtein_distance(\"description\", 2, 2)\n",
    "    .levenshtein_distance(\"name_description\", 1, 2)\n",
    "    .levenshtein_distance(\"description_name\", 2, 1)\n",
    "    .levenshtein_distance(\"name_manufacturer\", 1, 3)\n",
    "    .levenshtein_distance(\"description_manufacturer\", 2, 3)\n",
    ")\n",
    "\n",
    "ds = RecordLinkageDataSet(abt, buy, gt).attr_compare(cmp_config).cross_sources()\n",
    "display(ds.feature_matrix)"
   ],
   "id": "53ff31909914c378",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (1_180_452, 6)\n",
       "┌──────┬─────────────┬──────────────────┬──────────────────┬───────────────────┬───────────────────┐\n",
       "│ name ┆ description ┆ name_description ┆ description_name ┆ name_manufacturer ┆ description_manuf │\n",
       "│ ---  ┆ ---         ┆ ---              ┆ ---              ┆ ---               ┆ acturer           │\n",
       "│ i64  ┆ i64         ┆ i64              ┆ i64              ┆ i64               ┆ ---               │\n",
       "│      ┆             ┆                  ┆                  ┆                   ┆ i64               │\n",
       "╞══════╪═════════════╪══════════════════╪══════════════════╪═══════════════════╪═══════════════════╡\n",
       "│ 42   ┆ 171         ┆ 47               ┆ 171              ┆ 24                ┆ 190               │\n",
       "│ 35   ┆ 182         ┆ 22               ┆ 171              ┆ 24                ┆ 190               │\n",
       "│ 38   ┆ 167         ┆ 43               ┆ 168              ┆ 23                ┆ 187               │\n",
       "│ 64   ┆ 173         ┆ 32               ┆ 157              ┆ 24                ┆ 188               │\n",
       "│ 34   ┆ 168         ┆ 50               ┆ 168              ┆ 23                ┆ 187               │\n",
       "│ …    ┆ …           ┆ …                ┆ …                ┆ …                 ┆ …                 │\n",
       "│ 39   ┆ 297         ┆ 181              ┆ 338              ┆ 40                ┆ 356               │\n",
       "│ 46   ┆ 356         ┆ 40               ┆ 324              ┆ 37                ┆ 351               │\n",
       "│ 38   ┆ 356         ┆ 40               ┆ 343              ┆ 40                ┆ 355               │\n",
       "│ 39   ┆ 356         ┆ 40               ┆ 342              ┆ 40                ┆ 355               │\n",
       "│ 50   ┆ 356         ┆ 40               ┆ 334              ┆ 37                ┆ 352               │\n",
       "└──────┴─────────────┴──────────────────┴──────────────────┴───────────────────┴───────────────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_180_452, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>description</th><th>name_description</th><th>description_name</th><th>name_manufacturer</th><th>description_manufacturer</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>42</td><td>171</td><td>47</td><td>171</td><td>24</td><td>190</td></tr><tr><td>35</td><td>182</td><td>22</td><td>171</td><td>24</td><td>190</td></tr><tr><td>38</td><td>167</td><td>43</td><td>168</td><td>23</td><td>187</td></tr><tr><td>64</td><td>173</td><td>32</td><td>157</td><td>24</td><td>188</td></tr><tr><td>34</td><td>168</td><td>50</td><td>168</td><td>23</td><td>187</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>39</td><td>297</td><td>181</td><td>338</td><td>40</td><td>356</td></tr><tr><td>46</td><td>356</td><td>40</td><td>324</td><td>37</td><td>351</td></tr><tr><td>38</td><td>356</td><td>40</td><td>343</td><td>40</td><td>355</td></tr><tr><td>39</td><td>356</td><td>40</td><td>342</td><td>40</td><td>355</td></tr><tr><td>50</td><td>356</td><td>40</td><td>334</td><td>37</td><td>352</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T15:30:29.280114Z",
     "start_time": "2024-12-13T15:30:12.865122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = ds.feature_matrix.to_numpy()\n",
    "y = ds.target_vector.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "scaler.transform(X_train)\n",
    "folds = generate_k_folds(X_train, y_train, fold_count)\n",
    "\n",
    "stats = []\n",
    "for idx, (\n",
    "    X_fold_train,\n",
    "    y_fold_train,\n",
    "    X_fold_validation,\n",
    "    y_fold_validation,\n",
    ") in enumerate(folds, start=1):\n",
    "    trained_models = {\n",
    "        model: train(X_fold_train, y_fold_train) for model, train in models.items()\n",
    "    }\n",
    "    print(\"trained on fold #\", idx)\n",
    "\n",
    "    for model_name, model in trained_models.items():\n",
    "        prediction = model.predict(X_fold_validation)\n",
    "        f1 = f1_score(y_fold_validation, prediction, zero_division=0)\n",
    "        stats.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"precision\": precision_score(\n",
    "                    y_fold_validation, prediction, zero_division=0\n",
    "                ),\n",
    "                \"recall\": recall_score(y_fold_validation, prediction, zero_division=0),\n",
    "                \"f1\": f1,\n",
    "            }\n",
    "        )\n",
    "    print(\"evaluated fold #\", idx)\n",
    "\n",
    "stats = pl.DataFrame(stats)\n",
    "display(stats)\n",
    "avg_stats = stats.group_by(\"model\").mean()\n",
    "display(avg_stats)"
   ],
   "id": "b5a1dc8d6a940c61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained on fold # 1\n",
      "evaluated fold # 1\n",
      "trained on fold # 2\n",
      "evaluated fold # 2\n",
      "trained on fold # 3\n",
      "evaluated fold # 3\n",
      "trained on fold # 4\n",
      "evaluated fold # 4\n",
      "trained on fold # 5\n",
      "evaluated fold # 5\n",
      "trained on fold # 6\n",
      "evaluated fold # 6\n",
      "trained on fold # 7\n",
      "evaluated fold # 7\n",
      "trained on fold # 8\n",
      "evaluated fold # 8\n",
      "trained on fold # 9\n",
      "evaluated fold # 9\n",
      "trained on fold # 10\n",
      "evaluated fold # 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "shape: (20, 4)\n",
       "┌────────────┬───────────┬──────────┬──────────┐\n",
       "│ model      ┆ precision ┆ recall   ┆ f1       │\n",
       "│ ---        ┆ ---       ┆ ---      ┆ ---      │\n",
       "│ str        ┆ f64       ┆ f64      ┆ f64      │\n",
       "╞════════════╪═══════════╪══════════╪══════════╡\n",
       "│ svm        ┆ 0.0       ┆ 0.0      ┆ 0.0      │\n",
       "│ regression ┆ 0.588235  ┆ 0.147059 ┆ 0.235294 │\n",
       "│ svm        ┆ 0.0       ┆ 0.0      ┆ 0.0      │\n",
       "│ regression ┆ 0.3       ┆ 0.083333 ┆ 0.130435 │\n",
       "│ svm        ┆ 0.0       ┆ 0.0      ┆ 0.0      │\n",
       "│ …          ┆ …         ┆ …        ┆ …        │\n",
       "│ regression ┆ 0.578947  ┆ 0.142857 ┆ 0.229167 │\n",
       "│ svm        ┆ 0.0       ┆ 0.0      ┆ 0.0      │\n",
       "│ regression ┆ 0.35      ┆ 0.104478 ┆ 0.16092  │\n",
       "│ svm        ┆ 0.0       ┆ 0.0      ┆ 0.0      │\n",
       "│ regression ┆ 0.44      ┆ 0.157143 ┆ 0.231579 │\n",
       "└────────────┴───────────┴──────────┴──────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>precision</th><th>recall</th><th>f1</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;svm&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;regression&quot;</td><td>0.588235</td><td>0.147059</td><td>0.235294</td></tr><tr><td>&quot;svm&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;regression&quot;</td><td>0.3</td><td>0.083333</td><td>0.130435</td></tr><tr><td>&quot;svm&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;regression&quot;</td><td>0.578947</td><td>0.142857</td><td>0.229167</td></tr><tr><td>&quot;svm&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;regression&quot;</td><td>0.35</td><td>0.104478</td><td>0.16092</td></tr><tr><td>&quot;svm&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;regression&quot;</td><td>0.44</td><td>0.157143</td><td>0.231579</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "shape: (2, 4)\n",
       "┌────────────┬───────────┬──────────┬──────────┐\n",
       "│ model      ┆ precision ┆ recall   ┆ f1       │\n",
       "│ ---        ┆ ---       ┆ ---      ┆ ---      │\n",
       "│ str        ┆ f64       ┆ f64      ┆ f64      │\n",
       "╞════════════╪═══════════╪══════════╪══════════╡\n",
       "│ regression ┆ 0.484448  ┆ 0.121926 ┆ 0.193939 │\n",
       "│ svm        ┆ 0.0       ┆ 0.0      ┆ 0.0      │\n",
       "└────────────┴───────────┴──────────┴──────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>precision</th><th>recall</th><th>f1</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;regression&quot;</td><td>0.484448</td><td>0.121926</td><td>0.193939</td></tr><tr><td>&quot;svm&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We see a marked difference between SVMs and logistic regression. The number of\n",
    "features shouldn't be such a big factor because it's still small enough for\n",
    "SVMs to work properly.\n",
    "However, we can validate that logistic regression using co-referent attributes\n",
    "works properly here and in the other notebooks. The F1 score we can expect on\n",
    "Abt-Buy is around 0.20.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "This notebook reproduces the original findings in [@bilenko2002learning] to a\n",
    "sufficient degree to inspire confidence in the other results observed here.\n",
    "Using the Levenshtein distance instead of patterns or match/non-match semantics\n",
    "seems to improve both the precision and the recall of the matching process for\n",
    "both SVMs and Logistic Regression.\n",
    "But this is only an illusion!\n",
    "\n",
    "The improvement in precision, recall and F1 score are due to the shape/structure\n",
    "of the data.\n",
    "Running the same models on Abt-Buy, a much more challenging data set than the\n",
    "one used in the [@bilenko2002learning] paper, we get scores similar to our other\n",
    "notebooks that use Abt-Buy.\n",
    "This shows that the probability that entity resolution is correct doesn't\n",
    "increase with data partitioning choice or with using more attribute coreferences\n",
    "in our comparisons.\n",
    "Instead, the results of the entity resolution solution depend largely on the\n",
    "data where they are applied.\n",
    "\n",
    "Furthermore, we see that the current approach requires a _lot_ of data\n",
    "preprocessing to function correctly. For example, we have to compute a specific\n",
    "feature matrix which uses coreferences. Computing this matrix is by far the\n",
    "most expensive part of the training process.\n",
    "\n",
    "On top of this, the models:\n",
    "\n",
    "* can only compare apples to apples (that is, we can compare only the\n",
    "coreferences we define - if such a coreference can't be provided then the method\n",
    "doesn't work);\n",
    "* do not transfer any of what they learn to a new comparison.\n",
    "\n",
    "It's not obvious that we can get much further with coreferent attributes.\n",
    "Note that all the models which use coreferent attributes are just variations on\n",
    "the Fellegi-Sunter model.\n",
    "In order to get better results, we need to move to a different matching model\n",
    "which does not rely on coreferent attributes, but instead interprets the meaning\n",
    "of entity references.\n",
    "Logistic regression might still work with these models, but SVMs will not due to\n",
    "the higher dimensionality of these theoretical models."
   ],
   "id": "dcf60fa97d0a0d86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T14:11:30.520925Z",
     "start_time": "2024-12-13T14:11:30.519227Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "cb39889b6e3386e9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
