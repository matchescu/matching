{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Entity Resolution using Deep Learning\n",
    "\n",
    "In this notebook we're going to try to reproduce the results of some of the\n",
    "systems described in the most complete survey [@barlaugsurvey2021] on entity\n",
    "matching using deep learning techniques.\n",
    "Additionally, where we can, we will compare the results obtained using those\n",
    "techniques with logistic regression.\n",
    "\n",
    "First things first: a few imports of modules and data."
   ],
   "id": "fcfe9c9755b1e85c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:18.130250Z",
     "start_time": "2024-12-31T16:37:16.708108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matching.ml.datasets.test_blocking_dataset import block_engine\n",
    "!test -f ~/requirements.txt && pip install -r ~/requirements.txt"
   ],
   "id": "4f0433b0657bb2a6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:18.136557Z",
     "start_time": "2024-12-31T16:37:18.133978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matchescu.matching.entity_reference import RawComparison\n",
    "from matchescu.matching.ml.datasets import RecordLinkageDataSet"
   ],
   "id": "a20a39dc5c6a2361",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:18.253815Z",
     "start_time": "2024-12-31T16:37:18.252232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import polars as pl\n",
    "\n",
    "from matchescu.matching.extraction import CsvDataSource, Traits, ListDataSource"
   ],
   "id": "9114d8c81c1e5e1d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Just like we've done previously, we'll be using only the Abt-Buy dataset.",
   "id": "5e8c2ccd3a705121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:18.259427Z",
     "start_time": "2024-12-31T16:37:18.257587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LANG = \"en\"\n",
    "DATADIR = os.path.abspath(\"../../data\")\n",
    "LEFT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Abt.csv\")\n",
    "RIGHT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Buy.csv\")\n",
    "GROUND_TRUTH_PATH = os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\")"
   ],
   "id": "66d2c6464c3fcab6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Unlike with our previous approaches, we're not quite ready to construct a\n",
    "feature matrix.\n",
    "While we could definitely use the previous extraction traits to provide a\n",
    "feature matrix containing the similarities of co-referent attributes, we want to\n",
    "improve upon our work so far.\n",
    "To do so, we're going to attempt to implement Deepmatcher from scratch as it is\n",
    "described in the [@deepmatcher2018] paper.\n",
    "An important note is that we'll be implementing the Hybrid Deepmatcher approach\n",
    "because it is the one with the highest success.\n",
    "\n",
    "Firstly, we must embed attributes into sequences of word vectors.\n",
    "To do this we're going to use NLTK to tokenize the input and fasttext to create\n",
    "the character embeddings for every word."
   ],
   "id": "208649f40e5396cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:22.031198Z",
     "start_time": "2024-12-31T16:37:18.279953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fasttext\n",
    "import nltk\n",
    "\n",
    "from fasttext.util import download_model\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "download_model(LANG, if_exists=\"ignore\")\n",
    "ft_model = fasttext.load_model(f\"cc.{LANG}.300.bin\")"
   ],
   "id": "7e2d252685a99652",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cusi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have downloaded the `punkt` package locally, we can easily tokenize\n",
    "the words we find in each attribute in our two data sources.\n",
    "Let's write a function that does that."
   ],
   "id": "137d873065dd75b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:22.038568Z",
     "start_time": "2024-12-31T16:37:22.036388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model hyperparameters\n",
    "SIF_ALPHA = 1.0\n",
    "INPUT_SIZE = ft_model.get_dimension()"
   ],
   "id": "d36886ec19e64d4c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:22.059937Z",
     "start_time": "2024-12-31T16:37:22.055548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any, Generator, Iterable\n",
    "\n",
    "import torch\n",
    "from matchescu.typing import DataSource, Record\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenEmbedding:\n",
    "    token: str\n",
    "    embedding: torch.Tensor\n",
    "\n",
    "\n",
    "def tokenize_attribute_value(value: Any) -> Generator[str, None, None]:\n",
    "    if value is None:\n",
    "        return\n",
    "    if not isinstance(value, str):\n",
    "        value = str(value)\n",
    "    value = value.lower()\n",
    "    yield from nltk.word_tokenize(value)\n",
    "\n",
    "\n",
    "def embed_str(value: str) -> torch.Tensor:\n",
    "    return torch.from_numpy(ft_model.get_word_vector(value))\n",
    "\n",
    "\n",
    "def token_embedding(value: str) -> TokenEmbedding:\n",
    "    return TokenEmbedding(value, embed_str(value))\n",
    "\n",
    "\n",
    "def tokenize_words(record: tuple) -> list[list[TokenEmbedding]]:\n",
    "    return [\n",
    "        [token_embedding(token) for token in tokenize_attribute_value(value)]\n",
    "        for value in record\n",
    "    ]\n",
    "\n",
    "\n",
    "def tokenize_all(\n",
    "    records: Iterable[Record],\n",
    ") -> Generator[list[list[TokenEmbedding]], None, None]:\n",
    "    yield from (tokenize_words(record) for record in records)\n",
    "\n",
    "\n",
    "def build_normalized_unigram_frequencies(\n",
    "    processed_data_sources: Iterable[list[list[TokenEmbedding]]],\n",
    ") -> dict[str, float]:\n",
    "    token_frequencies = {}\n",
    "    word_count = 0\n",
    "    for ds in processed_data_sources:\n",
    "        for tokenized_record in ds:\n",
    "            for token_embedding in tokenized_record:\n",
    "                token_frequency = token_frequencies.get(token_embedding.token, 0)\n",
    "                token_frequency += 1\n",
    "                token_frequencies[token_embedding.token] = token_frequency\n",
    "                word_count += 1\n",
    "    return {\n",
    "        token: token_frequency / word_count\n",
    "        for token, token_frequency in token_frequencies.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_token_weights(\n",
    "    token_embeddings: list[TokenEmbedding],\n",
    "    frequency_table: dict[str, float],\n",
    "    a: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    return torch.Tensor(\n",
    "        [(a / (a + frequency_table.get(te.token, 1000000))) for te in token_embeddings]\n",
    "    )"
   ],
   "id": "c597555349a001b0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:22.081480Z",
     "start_time": "2024-12-31T16:37:22.062154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Hashable\n",
    "from matchescu.typing import EntityReference\n",
    "from matchescu.matching.blocking import BlockEngine\n",
    "\n",
    "\n",
    "abt_traits = Traits().int([0]).string([1, 2]).currency([3])\n",
    "abt = CsvDataSource(name=\"abt\", traits=abt_traits).read_csv(LEFT_CSV_PATH)\n",
    "buy_traits = Traits().int([0]).string([1, 2, 3]).currency([4])\n",
    "buy = CsvDataSource(name=\"buy\", traits=buy_traits).read_csv(RIGHT_CSV_PATH)\n",
    "gt = set(\n",
    "    pl.read_csv(\n",
    "        os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\"),\n",
    "        ignore_errors=True,\n",
    "    ).iter_rows()\n",
    ")\n",
    "\n",
    "\n",
    "def _id(ref: EntityReference) -> Hashable:\n",
    "    return ref[0]"
   ],
   "id": "e2462776ed478c7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:22.679270Z",
     "start_time": "2024-12-31T16:37:22.092185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_references = tokenize_all(itertools.chain(abt, buy))\n",
    "token_frequency_table = build_normalized_unigram_frequencies(all_references)\n",
    "print(\", \".join(itertools.islice(token_frequency_table, 10)))"
   ],
   "id": "50964571b6ae88e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552, sony, turntable, -, pslx350h, pslx350h/, belt, drive, system/, 33-1/3\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:23.236901Z",
     "start_time": "2024-12-31T16:37:22.687599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_engine = BlockEngine().add_source(abt, _id).add_source(buy, _id).tf_idf(0.26)\n",
    "block_engine.filter_candidates_jaccard(0.5)\n",
    "block_engine.update_candidate_pairs(False)\n",
    "metrics = block_engine.calculate_metrics(gt)\n",
    "\n",
    "print(\"Pair completeness:\", metrics.pair_completeness)\n",
    "print(\"Pair quality:\", metrics.pair_quality)\n",
    "print(\"Reduction ratio:\", metrics.reduction_ratio)"
   ],
   "id": "76b884e3d6ff2918",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair completeness: 0.9134001823154057\n",
      "Pair quality: 0.04418573885434581\n",
      "Reduction ratio: 0.9807895619644\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:23.249374Z",
     "start_time": "2024-12-31T16:37:23.245756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "class SifTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        frequency_table: dict[str, float],\n",
    "        alpha: float = 1.0,\n",
    "        input_dim: int = 300,\n",
    "        excluded_cols: list[int] = None,\n",
    "    ) -> None:\n",
    "        self._ft = frequency_table\n",
    "        self._a = alpha\n",
    "        self._n = input_dim\n",
    "        self._excluded_cols = set(excluded_cols or [])\n",
    "\n",
    "    def _create_tensor(self, attr_value: Any) -> torch.Tensor:\n",
    "        token_embeddings = list(\n",
    "            map(token_embedding, tokenize_attribute_value(attr_value))\n",
    "        )\n",
    "        if len(token_embeddings) < 1:\n",
    "            return torch.zeros(self._n)\n",
    "        token_weights = compute_token_weights(\n",
    "            token_embeddings, self._ft, self._a\n",
    "        ).reshape(len(token_embeddings), 1)\n",
    "        word_embeddings = torch.atleast_2d(\n",
    "            torch.stack([te.embedding for te in token_embeddings])\n",
    "        )\n",
    "        weighted_embeddings = token_weights * word_embeddings\n",
    "        colwise_weighted_sum = weighted_embeddings.sum(dim=0)\n",
    "        total_weight = token_weights.sum().float()\n",
    "        return colwise_weighted_sum / total_weight\n",
    "\n",
    "    def _transform(self, ref: EntityReference) -> Iterator:\n",
    "        for idx, value in enumerate(ref):\n",
    "            if idx not in self._excluded_cols:\n",
    "                yield self._create_tensor(value)\n",
    "            else:\n",
    "                yield value\n",
    "\n",
    "    def __call__(self, ref: EntityReference) -> EntityReference:\n",
    "        return tuple(self._transform(ref))"
   ],
   "id": "f9c64db1e90f8216",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:37:23.267442Z",
     "start_time": "2024-12-31T16:37:23.265433Z"
    }
   },
   "cell_type": "code",
   "source": "sif = SifTransform(token_frequency_table, SIF_ALPHA, INPUT_SIZE, excluded_cols=[0])",
   "id": "f8fb83adc8dae7b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:44:06.394708Z",
     "start_time": "2024-12-31T16:42:46.313450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml.datasets._blocking import BlockDataSet\n",
    "\n",
    "cmp_config = (\n",
    "    RawComparison()\n",
    "    .tensor_diff(\"name\", 1, 1)\n",
    "    .tensor_diff(\"description\", 2, 2)\n",
    "    .tensor_diff(\"name_manufacturer\", 1, 3)\n",
    "    .tensor_diff(\"description_manufacturer\", 2, 3)\n",
    "    .tensor_diff(\"price\", 3, 4)\n",
    ")\n",
    "\n",
    "ds = BlockDataSet(block_engine, gt, _id, _id).vector_compare(cmp_config)\n",
    "ds.transforms.append(sif)\n",
    "ds.cross_sources()\n",
    "\n",
    "X = ds.feature_matrix\n",
    "y = ds.target_vector.reshape((len(ds.target_vector), 1))\n",
    "print(X.shape, y.shape)\n",
    "print(len(y[y == 1]))"
   ],
   "id": "2b71123843515ecf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29155, 1500) (29155, 1)\n",
      "2327\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's add a few more helper functions that enable us to train an ANN classifier.",
   "id": "fd3a20893e4cd608"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:44:28.699073Z",
     "start_time": "2024-12-31T16:44:28.695868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.modules import loss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def get_torch_device():\n",
    "    return f\"mps:{torch.mps.device_count()-1}\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_torch_generator():\n",
    "    return torch.Generator(device=get_torch_device())\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    input_dataset: Dataset, batch_size: int = 32, shuffle: bool = True\n",
    "):\n",
    "    return DataLoader(\n",
    "        input_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "    )"
   ],
   "id": "5d5a242f79d196ce",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:44:29.648522Z",
     "start_time": "2024-12-31T16:44:29.341349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"total comparisons:\", len(X))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, stratify=y)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(\n",
    "    X_test, y_test, train_size=0.5, stratify=y_test\n",
    ")\n",
    "\n",
    "\n",
    "def match_non_match_ratio(arr):\n",
    "    k = np.count_nonzero(arr)\n",
    "    n = len(arr)\n",
    "    if n == k:\n",
    "        return 0\n",
    "    return round(k / (n - k), 5)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"training dataset size:\",\n",
    "    X_train.shape,\n",
    "    y_train.shape,\n",
    "    \"; match to non-match ratio:\",\n",
    "    match_non_match_ratio(y_train),\n",
    ")\n",
    "print(\n",
    "    \"cross-validation dataset size:\",\n",
    "    X_cv.shape,\n",
    "    y_cv.shape,\n",
    "    \"; match to non-match ratio:\",\n",
    "    match_non_match_ratio(y_cv),\n",
    ")\n",
    "print(\n",
    "    \"test dataset size:\",\n",
    "    X_test.shape,\n",
    "    y_test.shape,\n",
    "    \"; match to non-match ratio:\",\n",
    "    match_non_match_ratio(y_test),\n",
    ")"
   ],
   "id": "4b57fd67a0106405",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total comparisons: 29155\n",
      "training dataset size: (17493, 1500) (17493, 1) ; match to non-match ratio: 0.08672\n",
      "cross-validation dataset size: (5831, 1500) (5831, 1) ; match to non-match ratio: 0.08666\n",
      "test dataset size: (5831, 1500) (5831, 1) ; match to non-match ratio: 0.08686\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:44:37.300179Z",
     "start_time": "2024-12-31T16:44:37.291207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NpDataset(Dataset):\n",
    "    def __init__(self, features: np.ndarray, labels: np.ndarray):\n",
    "        self._features = features\n",
    "        self._targets = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self._features[idx], dtype=torch.float32)\n",
    "        targets = torch.tensor(self._targets[idx], dtype=torch.float32)\n",
    "        return features, targets\n",
    "\n",
    "\n",
    "train = NpDataset(X_train, y_train)\n",
    "cv = NpDataset(X_cv, y_cv)\n",
    "test = NpDataset(X_test, y_test)\n",
    "\n",
    "loss_function = loss.CrossEntropyLoss()"
   ],
   "id": "96b73d1b0edc1058",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:44:50.415425Z",
     "start_time": "2024-12-31T16:44:41.147713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml import TorchEngine\n",
    "from matchescu.matching.ml.modules import HighwayMatchClassifier\n",
    "\n",
    "N_EPOCHS = 3\n",
    "CLASSIFIER_INPUT_SIZE = len(cmp_config) * INPUT_SIZE\n",
    "TORCH_DEV = get_torch_device()\n",
    "\n",
    "print(\"comparison vector input size:\", CLASSIFIER_INPUT_SIZE)\n",
    "print(\"training on device:\", TORCH_DEV)\n",
    "\n",
    "matcher = HighwayMatchClassifier(CLASSIFIER_INPUT_SIZE)\n",
    "highway_engine = TorchEngine(\n",
    "    matcher, loss_function, Adam(params=matcher.parameters(), lr=1e-2), TORCH_DEV\n",
    ")\n",
    "highway_engine.train(create_dataloader(train), create_dataloader(cv), N_EPOCHS)"
   ],
   "id": "ed2fb3c226bb84f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparison vector input size: 1500\n",
      "training on device: mps:0\n",
      "Epoch 1/3, Train Loss: 0.3938, Val Loss: 0.3927\n",
      "Epoch 2/3, Train Loss: 0.3930, Val Loss: 0.3933\n",
      "Epoch 3/3, Train Loss: 0.3930, Val Loss: 0.3927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HighwayMatchClassifier(\n",
       "  (highway-net): HighwayNetwork(\n",
       "    (_scale_in): Linear(in_features=1500, out_features=512, bias=True)\n",
       "    (_layers): ModuleList(\n",
       "      (0-1): 2 x HighwayLayer(\n",
       "        (_basic_processor): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_transform_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (_scale_out): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T16:45:00.806077Z",
     "start_time": "2024-12-31T16:44:58.421713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss = highway_engine.evaluate(create_dataloader(test), compute_stats=True)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\n",
    "    \"Precision: %(precision).4f, Recall: %(recall).4f, F1: %(f1).4f\"\n",
    "    % highway_engine.stats\n",
    ")"
   ],
   "id": "44e98fbea7f24d05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3934\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cusi/Source/github.com/matchescu/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:41:22.417894Z",
     "start_time": "2024-12-27T13:39:43.905620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml.modules import ResidualMatchClassifier\n",
    "\n",
    "matcher = ResidualMatchClassifier(CLASSIFIER_INPUT_SIZE)\n",
    "residual_engine = TorchEngine(\n",
    "    matcher, loss_function, Adam(params=matcher.parameters(), lr=1e-3), TORCH_DEV\n",
    ")\n",
    "residual_engine.train(create_dataloader(train), create_dataloader(cv), 10)"
   ],
   "id": "90dd1216e1f1383b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.0290, Val Loss: 0.0206\n",
      "Epoch 2/10, Train Loss: 0.0237, Val Loss: 0.0144\n",
      "Epoch 3/10, Train Loss: 0.0164, Val Loss: 0.0173\n",
      "Epoch 4/10, Train Loss: 0.0165, Val Loss: 0.0142\n",
      "Epoch 5/10, Train Loss: 0.0207, Val Loss: 0.0138\n",
      "Epoch 6/10, Train Loss: 0.0147, Val Loss: 0.0223\n",
      "Epoch 7/10, Train Loss: 0.0136, Val Loss: 0.0145\n",
      "Epoch 8/10, Train Loss: 0.0114, Val Loss: 0.0162\n",
      "Epoch 9/10, Train Loss: 0.0117, Val Loss: 0.0163\n",
      "Epoch 10/10, Train Loss: 0.0115, Val Loss: 0.0264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResidualMatchClassifier(\n",
       "  (residual-net): ResidualNetwork(\n",
       "    (_ResidualNetwork__layers): ModuleList(\n",
       "      (0-1): 2 x Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (_ResidualNetwork__in_transform): Linear(in_features=1500, out_features=512, bias=True)\n",
       "  )\n",
       "  (transform-out): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:41:34.534403Z",
     "start_time": "2024-12-27T13:41:25.228838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss = residual_engine.evaluate(create_dataloader(test), compute_stats=True)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\n",
    "    \"Precision: %(precision).4f, Recall: %(recall).4f, F1: %(f1).4f\"\n",
    "    % residual_engine.stats\n",
    ")"
   ],
   "id": "ba2e0b090e632125",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0324\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3ac5c5d8a8cf290d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
