{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "x_x# Entity Resolution using Deep Learning\n",
    "\n",
    "In this notebook we're going to try to reproduce the results of some of the\n",
    "systems described in the most complete survey [@barlaugsurvey2021] on entity\n",
    "matching using deep learning techniques.\n",
    "Additionally, where we can, we will compare the results obtained using those\n",
    "techniques with logistic regression.\n",
    "\n",
    "First things first: a few imports of modules and data."
   ],
   "id": "fcfe9c9755b1e85c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:35:13.067568Z",
     "start_time": "2024-12-21T20:35:12.261327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from matchescu.matching.entity_reference import RawComparison\n",
    "!test -f ~/requirements.txt && pip install -r ~/requirements.txt"
   ],
   "id": "28afcff4bdff1caa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:35:13.137872Z",
     "start_time": "2024-12-21T20:35:13.072482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import polars as pl\n",
    "\n",
    "from matchescu.matching.ml.datasets import CsvDataSource"
   ],
   "id": "9114d8c81c1e5e1d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Just like we've done previously, we'll be using only the Abt-Buy dataset.",
   "id": "5e8c2ccd3a705121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:35:13.253658Z",
     "start_time": "2024-12-21T20:35:13.251901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LANG=\"en\"\n",
    "DATADIR = os.path.abspath(\"../../data\")\n",
    "LEFT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Abt.csv\")\n",
    "RIGHT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Buy.csv\")\n",
    "GROUND_TRUTH_PATH = os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\")"
   ],
   "id": "66d2c6464c3fcab6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Unlike with our previous approaches, we're not quite ready to construct a\n",
    "feature matrix.\n",
    "While we could definitely use the previous extraction traits to provide a\n",
    "feature matrix containing the similarities of co-referent attributes, we want to\n",
    "improve upon our work so far.\n",
    "To do so, we're going to attempt to implement Deepmatcher from scratch as it is\n",
    "described in the [@deepmatcher2018] paper.\n",
    "An important note is that we'll be implementing the Hybrid Deepmatcher approach\n",
    "because it is the one with the highest success.\n",
    "\n",
    "Firstly, we must embed attributes into sequences of word vectors.\n",
    "To do this we're going to use NLTK to tokenize the input and fasttext to create\n",
    "the character embeddings for every word."
   ],
   "id": "208649f40e5396cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:35:17.710175Z",
     "start_time": "2024-12-21T20:35:13.260380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fasttext\n",
    "import nltk\n",
    "\n",
    "from fasttext.util import download_model\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "download_model(LANG, if_exists=\"ignore\")\n",
    "ft_model = fasttext.load_model(f\"cc.{LANG}.300.bin\")"
   ],
   "id": "7e2d252685a99652",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cusi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have downloaded the `punkt` package locally, we can easily tokenize\n",
    "the words we find in each attribute in our two data sources.\n",
    "Let's write a function that does that."
   ],
   "id": "137d873065dd75b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:35:17.718838Z",
     "start_time": "2024-12-21T20:35:17.717292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model hyperparameters\n",
    "SIF_ALPHA = 1.0\n",
    "INPUT_SIZE = ft_model.get_dimension()"
   ],
   "id": "d36886ec19e64d4c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:35:18.679649Z",
     "start_time": "2024-12-21T20:35:17.724308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any, Generator, Iterable\n",
    "\n",
    "import torch\n",
    "from matchescu.typing import DataSource, Record\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenEmbedding:\n",
    "    token: str\n",
    "    embedding: torch.Tensor\n",
    "\n",
    "\n",
    "def tokenize_attribute_value(value: Any) -> Generator[str, None, None]:\n",
    "    if value is None:\n",
    "        return\n",
    "    if not isinstance(value, str):\n",
    "        value = str(value)\n",
    "    value = value.lower()\n",
    "    yield from nltk.word_tokenize(value)\n",
    "\n",
    "\n",
    "def embed_str(value: str) -> torch.Tensor:\n",
    "    return torch.from_numpy(ft_model.get_word_vector(value))\n",
    "\n",
    "\n",
    "def token_embedding(value: str) -> TokenEmbedding:\n",
    "    return TokenEmbedding(value, embed_str(value))\n",
    "\n",
    "\n",
    "def tokenize_words(record: tuple) -> list[list[TokenEmbedding]]:\n",
    "    return [\n",
    "        [\n",
    "            token_embedding(token)\n",
    "            for token in tokenize_attribute_value(value)\n",
    "        ]\n",
    "        for value in record\n",
    "    ]\n",
    "\n",
    "\n",
    "def tokenize_data_source(ds: DataSource[Record]) -> Generator[list[list[TokenEmbedding]], None, None]:\n",
    "    yield from (tokenize_words(record) for record in ds)\n",
    "\n",
    "\n",
    "def build_normalized_unigram_frequencies(processed_data_sources: Iterable[list[list[TokenEmbedding]]]) -> dict[str, float]:\n",
    "    token_frequencies = {}\n",
    "    word_count = 0\n",
    "    for ds in processed_data_sources:\n",
    "        for tokenized_record in ds:\n",
    "            for token_embedding in tokenized_record:\n",
    "                token_frequency = token_frequencies.get(token_embedding.token, 0)\n",
    "                token_frequency += 1\n",
    "                token_frequencies[token_embedding.token] = token_frequency\n",
    "                word_count += 1\n",
    "    return {\n",
    "        token: token_frequency / word_count\n",
    "        for token, token_frequency in token_frequencies.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_token_weights(token_embeddings: list[TokenEmbedding], frequency_table: dict[str, float], a: float = 1.0) -> torch.Tensor:\n",
    "    return torch.Tensor([\n",
    "        (a / (a + frequency_table[te.token]))\n",
    "        for te in token_embeddings\n",
    "    ])\n",
    "\n",
    "abt = CsvDataSource(name=\"abt\", traits=[]).read_csv(LEFT_CSV_PATH)\n",
    "buy = CsvDataSource(name=\"buy\", traits=[]).read_csv(RIGHT_CSV_PATH)\n",
    "tokenized_abt = list(tokenize_data_source(abt))\n",
    "tokenized_buy = list(tokenize_data_source(buy))\n",
    "token_frequency_table = build_normalized_unigram_frequencies(itertools.chain(tokenized_abt, tokenized_buy))"
   ],
   "id": "50964571b6ae88e0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:35:18.685710Z",
     "start_time": "2024-12-21T20:35:18.682682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SifTensorTrait:\n",
    "    def __init__(self, frequency_table: dict[str, float], alpha: float = 1.0, input_dim: int = 300) -> None:\n",
    "        self._ft = frequency_table\n",
    "        self._a = alpha\n",
    "        self._n = input_dim\n",
    "\n",
    "    def _extract_attribute_tensors(self, record: Record) -> Generator[torch.Tensor, None, None]:\n",
    "        for attr_value in record:\n",
    "            token_embeddings = list(map(token_embedding, tokenize_attribute_value(attr_value)))\n",
    "            if len(token_embeddings) < 1:\n",
    "                yield torch.zeros(self._n)\n",
    "                continue\n",
    "            token_weights = compute_token_weights(\n",
    "                token_embeddings, self._ft, self._a\n",
    "            ).reshape(len(token_embeddings), 1)\n",
    "            word_embeddings = torch.atleast_2d(\n",
    "                torch.stack([te.embedding for te in token_embeddings])\n",
    "            )\n",
    "            weighted_embeddings = token_weights * word_embeddings\n",
    "            colwise_weighted_sum = weighted_embeddings.sum(dim=0)\n",
    "            total_weight = token_weights.sum().float()\n",
    "            yield colwise_weighted_sum / total_weight\n",
    "\n",
    "    def __call__(self, record: Record) -> tuple[torch.Tensor, ...]:\n",
    "        return tuple(t for t in self._extract_attribute_tensors(record))\n"
   ],
   "id": "a1df039c1b00d2b4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:35:18.704979Z",
     "start_time": "2024-12-21T20:35:18.700973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sif = SifTensorTrait(token_frequency_table, SIF_ALPHA, INPUT_SIZE)\n",
    "abt = CsvDataSource(name=\"abt\", traits=[sif]).read_csv(LEFT_CSV_PATH)\n",
    "buy = CsvDataSource(name=\"buy\", traits=[sif]).read_csv(RIGHT_CSV_PATH)\n",
    "gt = set(\n",
    "    pl.read_csv(\n",
    "        os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\"),\n",
    "        ignore_errors=True,\n",
    "    ).iter_rows()\n",
    ")"
   ],
   "id": "76b884e3d6ff2918",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6fc4ca774c3221b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:35:53.397374Z",
     "start_time": "2024-12-21T20:35:18.719479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml.datasets import RecordLinkageDataSet\n",
    "cmp_config = (\n",
    "    RawComparison()\n",
    "    .euclidian_distance(\"name\", 1, 1)\n",
    "    .euclidian_distance(\"description\", 2, 2)\n",
    "    .euclidian_distance(\"name_manufacturer\", 1, 3)\n",
    "    .euclidian_distance(\"description_manufacturer\", 2, 3)\n",
    "    .euclidian_distance(\"price\", 3, 4)\n",
    ")\n",
    "\n",
    "ds = RecordLinkageDataSet(abt, buy, gt).attr_compare(cmp_config).cross_sources()\n",
    "display(ds.feature_matrix)"
   ],
   "id": "f9fb80ae5f21b671",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (1_180_452, 5)\n",
       "┌──────────┬─────────────┬───────────────────┬──────────────────────────┬──────────┐\n",
       "│ name     ┆ description ┆ name_manufacturer ┆ description_manufacturer ┆ price    │\n",
       "│ ---      ┆ ---         ┆ ---               ┆ ---                      ┆ ---      │\n",
       "│ f64      ┆ f64         ┆ f64               ┆ f64                      ┆ f64      │\n",
       "╞══════════╪═════════════╪═══════════════════╪══════════════════════════╪══════════╡\n",
       "│ 0.632966 ┆ 0.759937    ┆ 1.290251          ┆ 1.167136                 ┆ 0.000017 │\n",
       "│ 0.940538 ┆ 1.846598    ┆ 1.290251          ┆ 1.167136                 ┆ 0.000017 │\n",
       "│ 0.624203 ┆ 0.551165    ┆ 1.247905          ┆ 1.117197                 ┆ 0.000017 │\n",
       "│ 0.735849 ┆ 1.766785    ┆ 1.36398           ┆ 1.279892                 ┆ 0.000017 │\n",
       "│ 0.942468 ┆ 0.588245    ┆ 1.247905          ┆ 1.117197                 ┆ 0.000017 │\n",
       "│ …        ┆ …           ┆ …                 ┆ …                        ┆ …        │\n",
       "│ 0.692637 ┆ 0.844414    ┆ 1.831503          ┆ 1.760053                 ┆ 2.695225 │\n",
       "│ 1.1028   ┆ 0.530939    ┆ 0.924538          ┆ 0.872439                 ┆ 2.695225 │\n",
       "│ 0.996801 ┆ 0.530939    ┆ 1.342565          ┆ 1.245386                 ┆ 2.695225 │\n",
       "│ 0.835929 ┆ 0.530939    ┆ 1.342565          ┆ 1.245386                 ┆ 2.695225 │\n",
       "│ 0.791458 ┆ 0.530939    ┆ 0.770889          ┆ 0.684546                 ┆ 0.710628 │\n",
       "└──────────┴─────────────┴───────────────────┴──────────────────────────┴──────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_180_452, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>description</th><th>name_manufacturer</th><th>description_manufacturer</th><th>price</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.632966</td><td>0.759937</td><td>1.290251</td><td>1.167136</td><td>0.000017</td></tr><tr><td>0.940538</td><td>1.846598</td><td>1.290251</td><td>1.167136</td><td>0.000017</td></tr><tr><td>0.624203</td><td>0.551165</td><td>1.247905</td><td>1.117197</td><td>0.000017</td></tr><tr><td>0.735849</td><td>1.766785</td><td>1.36398</td><td>1.279892</td><td>0.000017</td></tr><tr><td>0.942468</td><td>0.588245</td><td>1.247905</td><td>1.117197</td><td>0.000017</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0.692637</td><td>0.844414</td><td>1.831503</td><td>1.760053</td><td>2.695225</td></tr><tr><td>1.1028</td><td>0.530939</td><td>0.924538</td><td>0.872439</td><td>2.695225</td></tr><tr><td>0.996801</td><td>0.530939</td><td>1.342565</td><td>1.245386</td><td>2.695225</td></tr><tr><td>0.835929</td><td>0.530939</td><td>1.342565</td><td>1.245386</td><td>2.695225</td></tr><tr><td>0.791458</td><td>0.530939</td><td>0.770889</td><td>0.684546</td><td>0.710628</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's add a few more helper functions that enable us to train an ANN classifier.",
   "id": "fd3a20893e4cd608"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:39:34.937675Z",
     "start_time": "2024-12-21T20:39:34.929265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.modules import loss\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def get_torch_device():\n",
    "    return f\"mps:{torch.mps.device_count()-1}\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_torch_generator():\n",
    "    return torch.Generator(device=get_torch_device())\n",
    "\n",
    "\n",
    "def create_dataloader(input_dataset: Dataset, batch_size: int = 32, shuffle: bool = True):\n",
    "    return DataLoader(\n",
    "        input_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "\n",
    "def random_split(input_dataset: Dataset, train_proportion: float = 0.6, cv_proportion = 0.2) -> tuple[Dataset, Dataset, Dataset]:\n",
    "    assert train_proportion + cv_proportion < 0.9\n",
    "\n",
    "    train, cv, test = torch.utils.data.random_split(\n",
    "        input_dataset,\n",
    "        [train_proportion, cv_proportion, 1.0 - train_proportion - cv_proportion],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    return train, cv, test\n",
    "\n",
    "\n",
    "def train_epoch(model, loss_fn, optimizer, torch_device, train_loader):\n",
    "    model.train(True)\n",
    "    total_loss = 0.0\n",
    "    for feats, labels in train_loader:\n",
    "        feats = feats.to(torch_device)\n",
    "        labels = labels.to(torch_device).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(feats)\n",
    "        loss_val = loss_fn(out, labels)\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss_val.item()\n",
    "\n",
    "    total_loss /= len(train_loader)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def evaluate_once(model, torch_device, loss_fn, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for feats, labels in data_loader:\n",
    "            feats = feats.to(torch_device)\n",
    "            labels = labels.to(torch_device).squeeze()\n",
    "\n",
    "            out = model(feats)\n",
    "            loss_val = loss_fn(out, labels)\n",
    "\n",
    "            total_loss += loss_val.item()\n",
    "    total_loss /= len(data_loader)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    x_validation_loader: DataLoader,\n",
    "    loss_fn: loss._Loss,\n",
    "    optimizer: Optimizer,\n",
    "    epochs: int = 10,\n",
    "    torch_device: str = \"cpu\",\n",
    "):\n",
    "    model.to(torch_device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, loss_fn, optimizer, torch_device, train_loader)\n",
    "        x_validation_loss = evaluate_once(model, torch_device, loss_fn, x_validation_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {x_validation_loss:.4f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "5d5a242f79d196ce",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:03:20.883307Z",
     "start_time": "2024-12-21T20:44:56.922119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml.modules import HighwayMatchClassifier\n",
    "\n",
    "N_EPOCHS = 10\n",
    "dev = get_torch_device()\n",
    "dataset = ds.create_torch_dataset()\n",
    "matcher = HighwayMatchClassifier(len(cmp_config))\n",
    "train, cv, test = random_split(dataset)\n",
    "\n",
    "loss_function = loss.CrossEntropyLoss()\n",
    "train_model(\n",
    "    matcher,\n",
    "    create_dataloader(train),\n",
    "    create_dataloader(cv),\n",
    "    loss_function,\n",
    "    Adam(matcher.parameters(), lr=0.01),\n",
    "    N_EPOCHS,\n",
    "    dev\n",
    ")"
   ],
   "id": "3987cd2bffa154cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3133, Val Loss: 0.3133\n",
      "Epoch 2/10, Train Loss: 0.3133, Val Loss: 0.3133\n",
      "Epoch 3/10, Train Loss: 0.3133, Val Loss: 0.3133\n",
      "Epoch 4/10, Train Loss: 0.3133, Val Loss: 0.3133\n",
      "Epoch 5/10, Train Loss: 0.3133, Val Loss: 0.3133\n",
      "Epoch 6/10, Train Loss: 0.3133, Val Loss: 0.3133\n",
      "Epoch 7/10, Train Loss: 0.3133, Val Loss: 0.3133\n",
      "Epoch 8/10, Train Loss: 0.3133, Val Loss: 0.3133\n",
      "Epoch 9/10, Train Loss: 0.3133, Val Loss: 0.3133\n",
      "Epoch 10/10, Train Loss: 0.3133, Val Loss: 0.3133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HighwayMatchClassifier(\n",
       "  (highway-net): HighwayNetwork(\n",
       "    (_scale_in): Linear(in_features=5, out_features=512, bias=True)\n",
       "    (_layers): ModuleList(\n",
       "      (0-1): 2 x HighwayLayer(\n",
       "        (_basic_processor): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_transform_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (_scale_out): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:15:13.405204Z",
     "start_time": "2024-12-21T21:15:00.392922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss = evaluate_once(matcher, dev, loss_function, create_dataloader(test))\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ],
   "id": "6bf286748a435925",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3133\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "90dd1216e1f1383b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
