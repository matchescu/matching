{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Entity Resolution using Deep Learning\n",
    "\n",
    "In this notebook we're going to try to reproduce the results of some of the\n",
    "systems described in the most complete survey [@barlaugsurvey2021] on entity\n",
    "matching using deep learning techniques.\n",
    "Additionally, where we can, we will compare the results obtained using those\n",
    "techniques with logistic regression.\n",
    "\n",
    "First things first: a few imports of modules and data."
   ],
   "id": "fcfe9c9755b1e85c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:33:35.796594Z",
     "start_time": "2024-12-27T13:33:34.974530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matchescu.data import EntityReferenceExtraction\n",
    "from matchescu.matching.entity_reference import RawComparison\n",
    "!test -f ~/requirements.txt && pip install -r ~/requirements.txt"
   ],
   "id": "28afcff4bdff1caa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:33:35.863591Z",
     "start_time": "2024-12-27T13:33:35.801794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import polars as pl\n",
    "\n",
    "from matchescu.data import EntityReferenceExtraction\n",
    "from matchescu.matching.extraction import CsvDataSource, Traits, ListDataSource"
   ],
   "id": "9114d8c81c1e5e1d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Just like we've done previously, we'll be using only the Abt-Buy dataset.",
   "id": "5e8c2ccd3a705121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:33:35.961240Z",
     "start_time": "2024-12-27T13:33:35.959424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LANG=\"en\"\n",
    "DATADIR = os.path.abspath(\"../../data\")\n",
    "LEFT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Abt.csv\")\n",
    "RIGHT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Buy.csv\")\n",
    "GROUND_TRUTH_PATH = os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\")"
   ],
   "id": "66d2c6464c3fcab6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Unlike with our previous approaches, we're not quite ready to construct a\n",
    "feature matrix.\n",
    "While we could definitely use the previous extraction traits to provide a\n",
    "feature matrix containing the similarities of co-referent attributes, we want to\n",
    "improve upon our work so far.\n",
    "To do so, we're going to attempt to implement Deepmatcher from scratch as it is\n",
    "described in the [@deepmatcher2018] paper.\n",
    "An important note is that we'll be implementing the Hybrid Deepmatcher approach\n",
    "because it is the one with the highest success.\n",
    "\n",
    "Firstly, we must embed attributes into sequences of word vectors.\n",
    "To do this we're going to use NLTK to tokenize the input and fasttext to create\n",
    "the character embeddings for every word."
   ],
   "id": "208649f40e5396cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:33:39.380617Z",
     "start_time": "2024-12-27T13:33:35.965141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fasttext\n",
    "import nltk\n",
    "\n",
    "from fasttext.util import download_model\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "download_model(LANG, if_exists=\"ignore\")\n",
    "ft_model = fasttext.load_model(f\"cc.{LANG}.300.bin\")"
   ],
   "id": "7e2d252685a99652",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cusi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have downloaded the `punkt` package locally, we can easily tokenize\n",
    "the words we find in each attribute in our two data sources.\n",
    "Let's write a function that does that."
   ],
   "id": "137d873065dd75b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:33:39.388233Z",
     "start_time": "2024-12-27T13:33:39.385728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model hyperparameters\n",
    "SIF_ALPHA = 1.0\n",
    "INPUT_SIZE = ft_model.get_dimension()"
   ],
   "id": "d36886ec19e64d4c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:33:39.474024Z",
     "start_time": "2024-12-27T13:33:39.426300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any, Generator, Iterable\n",
    "\n",
    "import torch\n",
    "from matchescu.typing import DataSource, Record\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenEmbedding:\n",
    "    token: str\n",
    "    embedding: torch.Tensor\n",
    "\n",
    "\n",
    "def tokenize_attribute_value(value: Any) -> Generator[str, None, None]:\n",
    "    if value is None:\n",
    "        return\n",
    "    if not isinstance(value, str):\n",
    "        value = str(value)\n",
    "    value = value.lower()\n",
    "    yield from nltk.word_tokenize(value)\n",
    "\n",
    "\n",
    "def embed_str(value: str) -> torch.Tensor:\n",
    "    return torch.from_numpy(ft_model.get_word_vector(value))\n",
    "\n",
    "\n",
    "def token_embedding(value: str) -> TokenEmbedding:\n",
    "    return TokenEmbedding(value, embed_str(value))\n",
    "\n",
    "\n",
    "def tokenize_words(record: tuple) -> list[list[TokenEmbedding]]:\n",
    "    return [\n",
    "        [\n",
    "            token_embedding(token)\n",
    "            for token in tokenize_attribute_value(value)\n",
    "        ]\n",
    "        for value in record\n",
    "    ]\n",
    "\n",
    "\n",
    "def tokenize_data_source(ds: DataSource[Record]) -> Generator[list[list[TokenEmbedding]], None, None]:\n",
    "    yield from (tokenize_words(record) for record in ds)\n",
    "\n",
    "\n",
    "def build_normalized_unigram_frequencies(processed_data_sources: Iterable[list[list[TokenEmbedding]]]) -> dict[str, float]:\n",
    "    token_frequencies = {}\n",
    "    word_count = 0\n",
    "    for ds in processed_data_sources:\n",
    "        for tokenized_record in ds:\n",
    "            for token_embedding in tokenized_record:\n",
    "                token_frequency = token_frequencies.get(token_embedding.token, 0)\n",
    "                token_frequency += 1\n",
    "                token_frequencies[token_embedding.token] = token_frequency\n",
    "                word_count += 1\n",
    "    return {\n",
    "        token: token_frequency / word_count\n",
    "        for token, token_frequency in token_frequencies.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_token_weights(token_embeddings: list[TokenEmbedding], frequency_table: dict[str, float], a: float = 1.0) -> torch.Tensor:\n",
    "    return torch.Tensor([\n",
    "        (a / (a + frequency_table[te.token]))\n",
    "        for te in token_embeddings\n",
    "    ])"
   ],
   "id": "4a29c2848273fb70",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:33:53.265759Z",
     "start_time": "2024-12-27T13:33:39.478101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.blocking import BlockEngine\n",
    "\n",
    "\n",
    "abt_traits = Traits().int([0]).string([1, 2]).currency([3])\n",
    "abt = CsvDataSource(name=\"abt\", traits=abt_traits).read_csv(LEFT_CSV_PATH)\n",
    "buy_traits = Traits().int([0]).string([1, 2, 3]).currency([4])\n",
    "buy = CsvDataSource(name=\"buy\", traits=buy_traits).read_csv(RIGHT_CSV_PATH)\n",
    "print(\"pre-blocking\", len(abt), len(buy))\n",
    "\n",
    "block_engine = (\n",
    "    BlockEngine(\n",
    "        [\n",
    "            EntityReferenceExtraction(abt, lambda x: x[0]),\n",
    "            EntityReferenceExtraction(buy, lambda x: x[0])\n",
    "        ]\n",
    "    )\n",
    "    .jaccard_blocks(1, 0.6)\n",
    "    .jaccard_blocks(2, 0.6)\n",
    "    .cross_sources_filter()\n",
    ")\n",
    "post_block_ds = block_engine.create_data_sources()\n",
    "abt = post_block_ds[\"abt\"]\n",
    "buy = post_block_ds[\"buy\"]\n",
    "print(\"post-blocking\", len(abt), len(buy))"
   ],
   "id": "837f79583ff8b5e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-blocking 1081 1092\n",
      "post-blocking 368 312\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:33:53.460390Z",
     "start_time": "2024-12-27T13:33:53.270604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_abt = list(tokenize_data_source(abt))\n",
    "tokenized_buy = list(tokenize_data_source(buy))\n",
    "token_frequency_table = build_normalized_unigram_frequencies(itertools.chain(tokenized_abt, tokenized_buy))"
   ],
   "id": "50964571b6ae88e0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:33:53.488686Z",
     "start_time": "2024-12-27T13:33:53.485524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SifTensorTrait:\n",
    "    def __init__(self, frequency_table: dict[str, float], alpha: float = 1.0, input_dim: int = 300) -> None:\n",
    "        self._ft = frequency_table\n",
    "        self._a = alpha\n",
    "        self._n = input_dim\n",
    "\n",
    "    def _extract_attribute_tensors(self, record: Record) -> Generator[torch.Tensor, None, None]:\n",
    "        for attr_value in record:\n",
    "            token_embeddings = list(map(token_embedding, tokenize_attribute_value(attr_value)))\n",
    "            if len(token_embeddings) < 1:\n",
    "                yield torch.zeros(self._n)\n",
    "                continue\n",
    "            token_weights = compute_token_weights(\n",
    "                token_embeddings, self._ft, self._a\n",
    "            ).reshape(len(token_embeddings), 1)\n",
    "            word_embeddings = torch.atleast_2d(\n",
    "                torch.stack([te.embedding for te in token_embeddings])\n",
    "            )\n",
    "            weighted_embeddings = token_weights * word_embeddings\n",
    "            colwise_weighted_sum = weighted_embeddings.sum(dim=0)\n",
    "            total_weight = token_weights.sum().float()\n",
    "            yield colwise_weighted_sum / total_weight\n",
    "\n",
    "    def __call__(self, record: Record) -> tuple[torch.Tensor, ...]:\n",
    "        return tuple(t for t in self._extract_attribute_tensors(record))\n"
   ],
   "id": "a1df039c1b00d2b4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:33:53.523895Z",
     "start_time": "2024-12-27T13:33:53.520259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sif = SifTensorTrait(token_frequency_table, SIF_ALPHA, INPUT_SIZE)\n",
    "traits = Traits().int([0])\n",
    "abt = ListDataSource(name=\"abt\", traits=[*traits, sif]).extend(abt)\n",
    "buy = ListDataSource(name=\"buy\", traits=[*traits, sif]).extend(buy)\n",
    "gt = set(\n",
    "    pl.read_csv(\n",
    "        os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\"),\n",
    "        ignore_errors=True,\n",
    "    ).iter_rows()\n",
    ")"
   ],
   "id": "76b884e3d6ff2918",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:37:57.124584Z",
     "start_time": "2024-12-27T13:33:53.563472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml.datasets import RecordLinkageDataSet\n",
    "cmp_config = (\n",
    "    RawComparison()\n",
    "    .tensor_diff(\"name\", 1, 1)\n",
    "    .tensor_diff(\"description\", 2, 2)\n",
    "    .tensor_diff(\"name_manufacturer\", 1, 3)\n",
    "    .tensor_diff(\"description_manufacturer\", 2, 3)\n",
    "    .tensor_diff(\"price\", 3, 4)\n",
    ")\n",
    "\n",
    "ds = RecordLinkageDataSet(abt, buy, gt).vector_compare(cmp_config).cross_sources()\n",
    "X = ds.feature_matrix.to_numpy()\n",
    "y = ds.target_vector.to_numpy()\n",
    "print(X.shape, y.shape)\n",
    "print(len(y[y==1]))"
   ],
   "id": "f9fb80ae5f21b671",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114816, 1500) (114816,)\n",
      "291\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's add a few more helper functions that enable us to train an ANN classifier.",
   "id": "fd3a20893e4cd608"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:37:57.146268Z",
     "start_time": "2024-12-27T13:37:57.143037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.modules import loss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def get_torch_device():\n",
    "    return f\"mps:{torch.mps.device_count()-1}\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_torch_generator():\n",
    "    return torch.Generator(device=get_torch_device())\n",
    "\n",
    "\n",
    "def create_dataloader(input_dataset: Dataset, batch_size: int = 32, shuffle: bool = True):\n",
    "    return DataLoader(\n",
    "        input_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "    )"
   ],
   "id": "5d5a242f79d196ce",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:37:58.406872Z",
     "start_time": "2024-12-27T13:37:57.187925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = ds.feature_matrix.to_numpy()\n",
    "y = ds.target_vector.to_numpy()\n",
    "print(\"total comparisons:\", len(X))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, stratify=y)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_test, y_test, train_size=0.5, stratify=y_test)\n",
    "\n",
    "\n",
    "def match_non_match_ratio(arr):\n",
    "    k = np.count_nonzero(arr)\n",
    "    n = len(arr)\n",
    "    if n == k:\n",
    "        return 0\n",
    "    return round(k / (n - k), 5)\n",
    "\n",
    "\n",
    "print(\"training dataset size:\", X_train.shape, y_train.shape,\"; match to non-match ratio:\", match_non_match_ratio(y_train))\n",
    "print(\"cross-validation dataset size:\", X_cv.shape, y_cv.shape,\"; match to non-match ratio:\", match_non_match_ratio(y_cv))\n",
    "print(\"test dataset size:\", X_test.shape, y_test.shape,\"; match to non-match ratio:\", match_non_match_ratio(y_test))"
   ],
   "id": "d80bd4639d15c0ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total comparisons: 114816\n",
      "training dataset size: (68889, 1500) (68889,) ; match to non-match ratio: 0.00255\n",
      "cross-validation dataset size: (22963, 1500) (22963,) ; match to non-match ratio: 0.00253\n",
      "test dataset size: (22964, 1500) (22964,) ; match to non-match ratio: 0.00253\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:37:58.412074Z",
     "start_time": "2024-12-27T13:37:58.409564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NpDataset(Dataset):\n",
    "    def __init__(self, features: np.ndarray, labels: np.ndarray):\n",
    "        self._features = features\n",
    "        self._targets = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self._features[idx], dtype=torch.float32)\n",
    "        targets = torch.tensor(self._targets[idx], dtype=torch.float32)\n",
    "        return features, targets\n",
    "\n",
    "\n",
    "train = NpDataset(X_train, y_train)\n",
    "cv = NpDataset(X_cv, y_cv)\n",
    "test = NpDataset(X_test, y_test)\n",
    "\n",
    "loss_function = loss.CrossEntropyLoss()"
   ],
   "id": "2cd20ca598c8ca18",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:38:35.721980Z",
     "start_time": "2024-12-27T13:37:58.429017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml import TorchEngine\n",
    "from matchescu.matching.ml.modules import HighwayMatchClassifier\n",
    "\n",
    "N_EPOCHS = 3\n",
    "CLASSIFIER_INPUT_SIZE = len(cmp_config) * INPUT_SIZE\n",
    "TORCH_DEV = get_torch_device()\n",
    "\n",
    "print(\"comparison vector input size:\", CLASSIFIER_INPUT_SIZE)\n",
    "print(\"training on device:\", TORCH_DEV)\n",
    "\n",
    "matcher = HighwayMatchClassifier(CLASSIFIER_INPUT_SIZE)\n",
    "highway_engine = TorchEngine(matcher, loss_function, Adam(params=matcher.parameters(), lr=1e-3), TORCH_DEV)\n",
    "highway_engine.train(create_dataloader(train), create_dataloader(cv), N_EPOCHS)"
   ],
   "id": "3987cd2bffa154cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparison vector input size: 1500\n",
      "training on device: mps:0\n",
      "Epoch 1/3, Train Loss: 0.3166, Val Loss: 0.3158\n",
      "Epoch 2/3, Train Loss: 0.3158, Val Loss: 0.3158\n",
      "Epoch 3/3, Train Loss: 0.3158, Val Loss: 0.3158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HighwayMatchClassifier(\n",
       "  (highway-net): HighwayNetwork(\n",
       "    (_scale_in): Linear(in_features=1500, out_features=512, bias=True)\n",
       "    (_layers): ModuleList(\n",
       "      (0-1): 2 x HighwayLayer(\n",
       "        (_basic_processor): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_transform_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (_scale_out): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:38:44.444516Z",
     "start_time": "2024-12-27T13:38:35.745609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss = highway_engine.evaluate(create_dataloader(test), compute_stats=True)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\"Precision: %(precision).4f, Recall: %(recall).4f, F1: %(f1).4f\" % highway_engine.stats)"
   ],
   "id": "6bf286748a435925",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3158\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cusi/Source/github.com/matchescu/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-27T13:38:44.556164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml.modules import ResidualMatchClassifier\n",
    "\n",
    "matcher = ResidualMatchClassifier(CLASSIFIER_INPUT_SIZE)\n",
    "residual_engine = TorchEngine(matcher, loss_function, Adam(params=matcher.parameters(), lr=1e-3), TORCH_DEV)\n",
    "residual_engine.train(create_dataloader(train), create_dataloader(cv), N_EPOCHS)"
   ],
   "id": "90dd1216e1f1383b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.0314, Val Loss: 0.0173\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_loss = residual_engine.evaluate(create_dataloader(test), compute_stats=True)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\"Precision: %(precision).4f, Recall: %(recall).4f, F1: %(f1).4f\" % residual_engine.stats)"
   ],
   "id": "ba2e0b090e632125"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
