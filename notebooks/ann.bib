@article{barlaugsurvey2021,
    author = {Barlaug, Nils and Gulla, Jon Atle},
    title = {Neural Networks for Entity Matching: A Survey},
    year = {2021},
    issue_date = {June 2021},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {15},
    number = {3},
    issn = {1556-4681},
    url = {https://doi.org/10.1145/3442200},
    doi = {10.1145/3442200},
    abstract = {Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there is still generous room for improvement. In recent years, we have seen new methods based upon deep learning techniques for natural language processing emerge. In this survey, we present how neural networks have been used for entity matching. Specifically, we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared to traditional methods, and propose a taxonomy of deep neural networks for entity matching.},
    journal = {ACM Trans. Knowl. Discov. Data},
    month = apr,
    articleno = {52},
    numpages = {37},
    keywords = {data matching, record linkage, entity resolution, entity matching, Deep learning}
}

@inproceedings{deepmatcher2018,
    author = {Mudgal, Sidharth and Li, Han and Rekatsinas, Theodoros and Doan, AnHai and Park, Youngchoon and Krishnan, Ganesh and Deep, Rohit and Arcaute, Esteban and Raghavendra, Vijay},
    title = {Deep Learning for Entity Matching: A Design Space Exploration},
    year = {2018},
    isbn = {9781450347037},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3183713.3196926},
    doi = {10.1145/3183713.3196926},
    abstract = {Entity matching (EM) finds data instances that refer to the same real-world entity. In this paper we examine applying deep learning (DL) to EM, to understand DL's benefits and limitations. We review many DL solutions that have been developed for related matching tasks in text processing (e.g., entity linking, textual entailment, etc.). We categorize these solutions and define a space of DL solutions for EM, as embodied by four solutions with varying representational power: SIF, RNN, Attention, and Hybrid. Next, we investigate the types of EM problems for which DL can be helpful. We consider three such problem types, which match structured data instances, textual instances, and dirty instances, respectively. We empirically compare the above four DL solutions with Magellan, a state-of-the-art learning-based EM solution. The results show that DL does not outperform current solutions on structured EM, but it can significantly outperform them on textual and dirty EM. For practitioners, this suggests that they should seriously consider using DL for textual and dirty EM problems. Finally, we analyze DL's performance and discuss future research directions.},
    booktitle = {Proceedings of the 2018 International Conference on Management of Data},
    pages = {19–34},
    numpages = {16},
    keywords = {entity resolution, entity matching, deep learning},
    location = {Houston, TX, USA},
    series = {SIGMOD '18}
}

@article{deeper2018,
    author = {Ebraheem, Muhammad and Thirumuruganathan, Saravanan and Joty, Shafiq and Ouzzani, Mourad and Tang, Nan},
    title = {Distributed representations of tuples for entity resolution},
    year = {2018},
    issue_date = {July 2018},
    publisher = {VLDB Endowment},
    volume = {11},
    number = {11},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/3236187.3236198},
    doi = {10.14778/3236187.3236198},
    abstract = {Despite the efforts in 70+ years in all aspects of entity resolution (ER), there is Łstill a high demand for democratizing ER - by reducing the heavy human involvement in labeling data, performing feature engineering, tuning parameters, and defining blocking functions. With the recent advances in deep learning, in particular distributed representations of words (a.k.a. word embeddings), we present a novel ER system, called DeepER, that achieves good accuracy, high efficiency, as well as ease-of-use (i.e., much less human efforts). We use sophisticated composition methods, namely uni- and bi-directional recurrent neural networks (RNNs) with long short term memory (LSTM) hidden units, to convert each tuple to a distributed representation (i.e., a vector), which can in turn be used to effectively capture similarities between tuples. We consider both the case where pre-trained word embeddings are available as well the case where they are not; we present ways to learn and tune the distributed representations that are customized for a specific ER task under different scenarios. We propose a locality sensitive hashing (LSH) based blocking approach that takes all attributes of a tuple into consideration and produces much smaller blocks, compared with traditional methods that consider only a few attributes. We evaluate our algorithms on multiple datasets (including benchmarks, biomedical data, as well as multi-lingual data) and the extensive experimental results show that DeepER outperforms existing solutions.},
    journal = {Proc. VLDB Endow.},
    month = jul,
    pages = {1454–1467},
    numpages = {14}
}