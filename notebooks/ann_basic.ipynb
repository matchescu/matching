{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Entity Resolution using Artificial Neural Networks\n",
    "\n",
    "Another very popular implementation choice for entity resolution is a standard\n",
    "classification network.\n",
    "In this notebook we're looking into the difference between training a feed\n",
    "forward classifier using synthetic comparison data and vector data obtained\n",
    "using pre-defined character-level word embeddings."
   ],
   "id": "fcfe9c9755b1e85c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:24:56.809419Z",
     "start_time": "2025-01-05T10:24:55.361104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matching.ml.datasets.test_blocking_dataset import block_engine\n",
    "!test -f ~/requirements.txt && pip install -r ~/requirements.txt"
   ],
   "id": "4f0433b0657bb2a6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:24:56.815555Z",
     "start_time": "2025-01-05T10:24:56.813139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matchescu.matching.entity_reference import RawComparison"
   ],
   "id": "a20a39dc5c6a2361",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:24:56.937124Z",
     "start_time": "2025-01-05T10:24:56.935416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import polars as pl\n",
    "\n",
    "from matchescu.matching.extraction import CsvDataSource, Traits"
   ],
   "id": "9114d8c81c1e5e1d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Most of our other notebooks use the Abt-Buy dataset and we're going to use it\n",
    "here, too."
   ],
   "id": "5e8c2ccd3a705121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:24:56.943123Z",
     "start_time": "2025-01-05T10:24:56.941302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LANG = \"en\"\n",
    "DATADIR = os.path.abspath(\"../../data\")\n",
    "LEFT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Abt.csv\")\n",
    "RIGHT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Buy.csv\")\n",
    "GROUND_TRUTH_PATH = os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\")"
   ],
   "id": "66d2c6464c3fcab6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's create some word embeddings using the `fasttext` library.\n",
    "We need to download a prebuilt model."
   ],
   "id": "208649f40e5396cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:25:01.309674Z",
     "start_time": "2025-01-05T10:24:56.947086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fasttext\n",
    "\n",
    "from fasttext.util import download_model\n",
    "\n",
    "\n",
    "download_model(LANG, if_exists=\"ignore\")\n",
    "ft_model = fasttext.load_model(f\"cc.{LANG}.300.bin\")"
   ],
   "id": "7e2d252685a99652",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When computing the word embeddings, we're going to account for the training\n",
    "dataset's particularities by using an algorithm similar to SIF (smooth inverse\n",
    "frequency) in a similar way [@deepmatcher] uses it."
   ],
   "id": "137d873065dd75b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:25:01.316015Z",
     "start_time": "2025-01-05T10:25:01.314258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Model hyperparameters\n",
    "SIF_ALPHA = 1.0\n",
    "INPUT_SIZE = ft_model.get_dimension()\n",
    "TOKEN_RE = re.compile(r'[\\W_]+', re.IGNORECASE)"
   ],
   "id": "d36886ec19e64d4c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's implement some helper functions that help us with determining SIF.",
   "id": "fe3ea834f4a262e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:25:01.324308Z",
     "start_time": "2025-01-05T10:25:01.319888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any, Generator, Iterable\n",
    "\n",
    "import torch\n",
    "from matchescu.typing import Record\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenEmbedding:\n",
    "    token: str\n",
    "    embedding: torch.Tensor\n",
    "\n",
    "\n",
    "def tokenize_attribute_value(value: Any) -> Generator[str, None, None]:\n",
    "    if value is None:\n",
    "        return\n",
    "    if not isinstance(value, str):\n",
    "        value = str(value)\n",
    "    value = value.lower()\n",
    "    yield from TOKEN_RE.split(value)\n",
    "\n",
    "\n",
    "def embed_str(value: str) -> torch.Tensor:\n",
    "    return torch.from_numpy(ft_model.get_word_vector(value))\n",
    "\n",
    "\n",
    "def token_embedding(value: str) -> TokenEmbedding:\n",
    "    return TokenEmbedding(value, embed_str(value))\n",
    "\n",
    "\n",
    "def tokenize_words(record: tuple) -> list[list[TokenEmbedding]]:\n",
    "    return [\n",
    "        [token_embedding(token) for token in tokenize_attribute_value(value)]\n",
    "        for value in record\n",
    "    ]\n",
    "\n",
    "\n",
    "def tokenize_all(\n",
    "    records: Iterable[Record],\n",
    ") -> Generator[list[list[TokenEmbedding]], None, None]:\n",
    "    yield from (tokenize_words(record) for record in records)\n",
    "\n",
    "\n",
    "def build_normalized_unigram_frequencies(\n",
    "    processed_data_sources: Iterable[list[list[TokenEmbedding]]],\n",
    ") -> dict[str, float]:\n",
    "    token_frequencies = {}\n",
    "    word_count = 0\n",
    "    for ds in processed_data_sources:\n",
    "        for tokenized_record in ds:\n",
    "            for token_embedding in tokenized_record:\n",
    "                token_frequency = token_frequencies.get(token_embedding.token, 0)\n",
    "                token_frequency += 1\n",
    "                token_frequencies[token_embedding.token] = token_frequency\n",
    "                word_count += 1\n",
    "    return {\n",
    "        token: token_frequency / word_count\n",
    "        for token, token_frequency in token_frequencies.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_token_weights(\n",
    "    token_embeddings: list[TokenEmbedding],\n",
    "    frequency_table: dict[str, float],\n",
    "    a: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    return torch.Tensor(\n",
    "        [(a / (a + frequency_table.get(te.token, 1000000))) for te in token_embeddings]\n",
    "    )"
   ],
   "id": "c597555349a001b0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we define our data sources so we can extract entity references.",
   "id": "6981536c0b73ba0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:25:01.347763Z",
     "start_time": "2025-01-05T10:25:01.328631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Hashable\n",
    "from matchescu.typing import EntityReference\n",
    "from matchescu.matching.blocking import BlockEngine\n",
    "\n",
    "\n",
    "abt_traits = Traits().int([0]).string([1, 2]).currency([3])\n",
    "abt = CsvDataSource(name=\"abt\", traits=abt_traits).read_csv(LEFT_CSV_PATH)\n",
    "buy_traits = Traits().int([0]).string([1, 2, 3]).currency([4])\n",
    "buy = CsvDataSource(name=\"buy\", traits=buy_traits).read_csv(RIGHT_CSV_PATH)\n",
    "gt = set(\n",
    "    pl.read_csv(\n",
    "        os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\"),\n",
    "        ignore_errors=True,\n",
    "    ).iter_rows()\n",
    ")\n",
    "\n",
    "\n",
    "def _id(ref: EntityReference) -> Hashable:\n",
    "    return ref[0]"
   ],
   "id": "e2462776ed478c7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We know from our logistic regression notebook that blocking and filtering both\n",
    "dramatically improve training effectiveness and speed up performance.\n",
    "In this next step, we extract candidate pairs without sacrificing too much of\n",
    "the comparison quality in relation to the ground truth."
   ],
   "id": "7bb1fb688c8867ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:25:01.958892Z",
     "start_time": "2025-01-05T10:25:01.366364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_engine = BlockEngine().add_source(abt, _id).add_source(buy, _id).tf_idf(0.26)\n",
    "block_engine.filter_candidates_jaccard(0.5)\n",
    "block_engine.update_candidate_pairs(False)\n",
    "metrics = block_engine.calculate_metrics(gt)\n",
    "\n",
    "print(\"Pair completeness:\", metrics.pair_completeness)\n",
    "print(\"Pair quality:\", metrics.pair_quality)\n",
    "print(\"Reduction ratio:\", metrics.reduction_ratio)"
   ],
   "id": "76b884e3d6ff2918",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair completeness: 0.9134001823154057\n",
      "Pair quality: 0.04418573885434581\n",
      "Reduction ratio: 0.9807895619644\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:25:07.762928Z",
     "start_time": "2025-01-05T10:25:01.964599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_references = tokenize_all(block_engine)\n",
    "token_frequency_table = build_normalized_unigram_frequencies(all_references)\n",
    "print(\", \".join(f\"{k}={round(v, 4)}\" for k, v in itertools.islice(token_frequency_table.items(), 10)))"
   ],
   "id": "50964571b6ae88e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552=0.0, sony=0.0077, turntable=0.0003, pslx350h=0.0, belt=0.0002, drive=0.0014, system=0.0052, 33=0.0001, 1=0.0064, 3=0.0051\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:25:07.794873Z",
     "start_time": "2025-01-05T10:25:07.791256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "class SifTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        frequency_table: dict[str, float],\n",
    "        alpha: float = 1.0,\n",
    "        input_dim: int = 300,\n",
    "        excluded_cols: list[int] = None,\n",
    "    ) -> None:\n",
    "        self._ft = frequency_table\n",
    "        self._a = alpha\n",
    "        self._n = input_dim\n",
    "        self._excluded_cols = set(excluded_cols or [])\n",
    "\n",
    "    def _create_tensor(self, attr_value: Any) -> torch.Tensor:\n",
    "        token_embeddings = list(\n",
    "            map(token_embedding, tokenize_attribute_value(attr_value))\n",
    "        )\n",
    "        if len(token_embeddings) < 1:\n",
    "            return torch.zeros(self._n)\n",
    "        token_weights = compute_token_weights(\n",
    "            token_embeddings, self._ft, self._a\n",
    "        ).reshape(len(token_embeddings), 1)\n",
    "        word_embeddings = torch.atleast_2d(\n",
    "            torch.stack([te.embedding for te in token_embeddings])\n",
    "        )\n",
    "        weighted_embeddings = token_weights * word_embeddings\n",
    "        colwise_weighted_sum = weighted_embeddings.sum(dim=0)\n",
    "        total_weight = token_weights.sum().float()\n",
    "        return colwise_weighted_sum / total_weight\n",
    "\n",
    "    def _transform(self, ref: EntityReference) -> Iterator:\n",
    "        for idx, value in enumerate(ref):\n",
    "            if idx not in self._excluded_cols:\n",
    "                yield self._create_tensor(value)\n",
    "            else:\n",
    "                yield value\n",
    "\n",
    "    def __call__(self, ref: EntityReference) -> EntityReference:\n",
    "        return tuple(self._transform(ref))"
   ],
   "id": "f9c64db1e90f8216",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:25:07.803374Z",
     "start_time": "2025-01-05T10:25:07.801905Z"
    }
   },
   "cell_type": "code",
   "source": "sif = SifTransform(token_frequency_table, SIF_ALPHA, INPUT_SIZE, excluded_cols=[0])",
   "id": "f8fb83adc8dae7b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's add a few more helper functions that enable us to train an ANN classifier.",
   "id": "fd3a20893e4cd608"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:39:50.531127Z",
     "start_time": "2025-01-05T10:39:50.528402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def get_torch_device():\n",
    "    return f\"mps:{torch.mps.device_count()-1}\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_torch_generator():\n",
    "    return torch.Generator(device=get_torch_device())\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    input_dataset: Dataset, batch_size: int = 32, shuffle: bool = True\n",
    "):\n",
    "    return DataLoader(\n",
    "        input_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "    )"
   ],
   "id": "5d5a242f79d196ce",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:39:53.543414Z",
     "start_time": "2025-01-05T10:39:53.539835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NpDataset(Dataset):\n",
    "    def __init__(self, features: np.ndarray, labels: np.ndarray):\n",
    "        self._features = features\n",
    "        self._targets = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self._features[idx], dtype=torch.float32)\n",
    "        targets = torch.tensor(self._targets[idx], dtype=torch.float32)\n",
    "        return features, targets"
   ],
   "id": "96b73d1b0edc1058",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T11:25:27.079361Z",
     "start_time": "2025-01-05T11:25:27.076588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class FFClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self._layers = torch.nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        return self._layers(input_tensor)"
   ],
   "id": "168b2a7a7b7df1f2",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T10:39:52.163318Z",
     "start_time": "2025-01-05T10:39:52.100395Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total comparisons: 29155\n",
      "training dataset size: (17493, 5) (17493, 1) ; match to non-match ratio: 0.08672\n",
      "cross-validation dataset size: (5831, 5) (5831, 1) ; match to non-match ratio: 0.08666\n",
      "test dataset size: (5831, 5) (5831, 1) ; match to non-match ratio: 0.08686\n"
     ]
    }
   ],
   "execution_count": 59,
   "source": [
    "from matchescu.matching.ml.datasets._blocking import BlockDataSet\n",
    "\n",
    "\n",
    "def match_non_match_ratio(arr):\n",
    "    k = np.count_nonzero(arr)\n",
    "    n = len(arr)\n",
    "    if n == k:\n",
    "        return 0\n",
    "    return round(k / (n - k), 5)\n",
    "\n",
    "\n",
    "attr_cmp_config = (\n",
    "    RawComparison()\n",
    "    .levenshtein(\"name\", 1, 1)\n",
    "    .levenshtein(\"description\", 2, 2)\n",
    "    .levenshtein(\"name_manufacturer\", 1, 3)\n",
    "    .levenshtein(\"description_manufacturer\", 2, 3)\n",
    "    .diff(\"price\", 3, 4)\n",
    ")\n",
    "attr_ds = BlockDataSet(block_engine, gt, _id, _id).attr_compare(attr_cmp_config)\n",
    "attr_ds.cross_sources()\n",
    "\n",
    "X = attr_ds.feature_matrix\n",
    "y = attr_ds.target_vector.reshape(\n",
    "    len(attr_ds.target_vector), 1\n",
    ")\n",
    "print(X.shape, y.shape)\n",
    "print(len(y[y == 1]))\n",
    "print(\"total comparisons:\", len(X))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, stratify=y)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(\n",
    "    X_test, y_test, train_size=0.5, stratify=y_test\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"training dataset size:\",\n",
    "    X_train.shape,\n",
    "    y_train.shape,\n",
    "    \"; match to non-match ratio:\",\n",
    "    match_non_match_ratio(y_train),\n",
    ")\n",
    "print(\n",
    "    \"cross-validation dataset size:\",\n",
    "    X_cv.shape,\n",
    "    y_cv.shape,\n",
    "    \"; match to non-match ratio:\",\n",
    "    match_non_match_ratio(y_cv),\n",
    ")\n",
    "print(\n",
    "    \"test dataset size:\",\n",
    "    X_test.shape,\n",
    "    y_test.shape,\n",
    "    \"; match to non-match ratio:\",\n",
    "    match_non_match_ratio(y_test),\n",
    ")"
   ],
   "id": "4b57fd67a0106405"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T11:25:56.924247Z",
     "start_time": "2025-01-05T11:25:41.301618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.matching.ml import TorchEngine\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "TORCH_DEV = get_torch_device()\n",
    "\n",
    "train = NpDataset(X_train, y_train)\n",
    "cv = NpDataset(X_cv, y_cv)\n",
    "test = NpDataset(X_test, y_test)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "print(\"comparison vector input size:\", len(attr_cmp_config))\n",
    "print(\"training on device:\", TORCH_DEV)\n",
    "\n",
    "matcher = FFClassifier(len(attr_cmp_config), 10)\n",
    "attr_engine = TorchEngine(\n",
    "    matcher, loss_function, Adam(params=matcher.parameters(), lr=1e-3), TORCH_DEV\n",
    ")\n",
    "attr_engine.train(create_dataloader(train), create_dataloader(cv), N_EPOCHS)"
   ],
   "id": "ed2fb3c226bb84f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparison vector input size: 5\n",
      "training on device: mps:0\n",
      "Epoch 1/10, Train Loss: 0.4002, Val Loss: 0.2866\n",
      "Epoch 2/10, Train Loss: 0.2793, Val Loss: 0.2726\n",
      "Epoch 3/10, Train Loss: 0.2631, Val Loss: 0.2559\n",
      "Epoch 4/10, Train Loss: 0.2461, Val Loss: 0.2569\n",
      "Epoch 5/10, Train Loss: 0.2328, Val Loss: 0.2374\n",
      "Epoch 6/10, Train Loss: 0.2380, Val Loss: 0.2267\n",
      "Epoch 7/10, Train Loss: 0.2238, Val Loss: 0.2260\n",
      "Epoch 8/10, Train Loss: 0.2230, Val Loss: 0.2221\n",
      "Epoch 9/10, Train Loss: 0.2221, Val Loss: 0.2261\n",
      "Epoch 10/10, Train Loss: 0.2314, Val Loss: 0.2256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FFClassifier(\n",
       "  (_layers): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=8, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_loss = attr_engine.evaluate(create_dataloader(test), compute_stats=True)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\n",
    "    \"Precision: %(precision).4f, Recall: %(recall).4f, F1: %(f1).4f\"\n",
    "    % attr_engine.stats\n",
    ")"
   ],
   "id": "24c863c0f99ca615",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T11:32:00.282200Z",
     "start_time": "2025-01-05T11:30:39.601464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_cmp_config = (\n",
    "    RawComparison()\n",
    "    .tensor_diff(\"name\", 1, 1)\n",
    "    .tensor_diff(\"description\", 2, 2)\n",
    "    .tensor_diff(\"name_manufacturer\", 1, 3)\n",
    "    .tensor_diff(\"description_manufacturer\", 2, 3)\n",
    "    .tensor_diff(\"price\", 3, 4)\n",
    ")\n",
    "vector_ds = BlockDataSet(block_engine, gt, _id, _id).vector_compare(vector_cmp_config)\n",
    "vector_ds.transforms.append(sif)\n",
    "vector_ds.cross_sources()\n",
    "\n",
    "X = vector_ds.feature_matrix\n",
    "y = vector_ds.target_vector.reshape(\n",
    "    len(vector_ds.target_vector), 1\n",
    ")\n",
    "print(X.shape, y.shape)\n",
    "print(len(y[y == 1]))\n",
    "print(\"total comparisons:\", len(X))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, stratify=y)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(\n",
    "    X_test, y_test, train_size=0.5, stratify=y_test\n",
    ")\n",
    "\n",
    "\n",
    "def match_non_match_ratio(arr):\n",
    "    k = np.count_nonzero(arr)\n",
    "    n = len(arr)\n",
    "    if n == k:\n",
    "        return 0\n",
    "    return round(k / (n - k), 5)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"training dataset size:\",\n",
    "    X_train.shape,\n",
    "    y_train.shape,\n",
    "    \"; match to non-match ratio:\",\n",
    "    match_non_match_ratio(y_train),\n",
    ")\n",
    "print(\n",
    "    \"cross-validation dataset size:\",\n",
    "    X_cv.shape,\n",
    "    y_cv.shape,\n",
    "    \"; match to non-match ratio:\",\n",
    "    match_non_match_ratio(y_cv),\n",
    ")\n",
    "print(\n",
    "    \"test dataset size:\",\n",
    "    X_test.shape,\n",
    "    y_test.shape,\n",
    "    \"; match to non-match ratio:\",\n",
    "    match_non_match_ratio(y_test),\n",
    ")"
   ],
   "id": "44e98fbea7f24d05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29155, 1500) (29155, 1)\n",
      "2327\n",
      "total comparisons: 29155\n",
      "training dataset size: (17493, 1500) (17493, 1) ; match to non-match ratio: 0.08672\n",
      "cross-validation dataset size: (5831, 1500) (5831, 1) ; match to non-match ratio: 0.08666\n",
      "test dataset size: (5831, 1500) (5831, 1) ; match to non-match ratio: 0.08686\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T11:40:08.366809Z",
     "start_time": "2025-01-05T11:39:51.829521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLASSIFIER_INPUT_SIZE = ft_model.get_dimension()*len(vector_cmp_config)\n",
    "N_EPOCHS = 10\n",
    "TORCH_DEV = get_torch_device()\n",
    "\n",
    "train = NpDataset(X_train, y_train)\n",
    "cv = NpDataset(X_cv, y_cv)\n",
    "test = NpDataset(X_test, y_test)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "print(\"comparison vector input size:\", len(attr_cmp_config))\n",
    "print(\"training on device:\", TORCH_DEV)\n",
    "\n",
    "vector_matcher = FFClassifier(CLASSIFIER_INPUT_SIZE, 100)\n",
    "vector_engine = TorchEngine(\n",
    "    vector_matcher, loss_function, Adam(params=vector_matcher.parameters(), lr=5e-3), TORCH_DEV\n",
    ")\n",
    "vector_engine.train(create_dataloader(train), create_dataloader(cv), 10)"
   ],
   "id": "90dd1216e1f1383b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparison vector input size: 5\n",
      "training on device: mps:0\n",
      "Epoch 1/10, Train Loss: 0.2720, Val Loss: 0.2581\n",
      "Epoch 2/10, Train Loss: 0.2501, Val Loss: 0.2510\n",
      "Epoch 3/10, Train Loss: 0.2346, Val Loss: 0.2471\n",
      "Epoch 4/10, Train Loss: 0.2225, Val Loss: 0.2423\n",
      "Epoch 5/10, Train Loss: 0.2085, Val Loss: 0.2429\n",
      "Epoch 6/10, Train Loss: 0.1987, Val Loss: 0.2495\n",
      "Epoch 7/10, Train Loss: 0.1881, Val Loss: 0.2370\n",
      "Epoch 8/10, Train Loss: 0.1784, Val Loss: 0.2320\n",
      "Epoch 9/10, Train Loss: 0.1662, Val Loss: 0.2346\n",
      "Epoch 10/10, Train Loss: 0.1559, Val Loss: 0.2445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FFClassifier(\n",
       "  (_layers): Sequential(\n",
       "    (0): Linear(in_features=1500, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=8, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T11:40:24.525655Z",
     "start_time": "2025-01-05T11:40:22.264873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss = vector_engine.evaluate(create_dataloader(test), compute_stats=True)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\n",
    "    \"Precision: %(precision).4f, Recall: %(recall).4f, F1: %(f1).4f\"\n",
    "    % vector_engine.stats\n",
    ")"
   ],
   "id": "ba2e0b090e632125",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2470\n",
      "Precision: 0.5545, Recall: 0.2618, F1: 0.3557\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see a marked improvement over the attribute comparisons using simple\n",
    "distance metrics."
   ],
   "id": "9771ad24a6abe3a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3ac5c5d8a8cf290d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
